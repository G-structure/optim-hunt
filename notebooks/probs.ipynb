{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_hunter.llama_model import load_llama_model\n",
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "import torch\n",
    "from einops import einsum\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_number_probabilities(model, prompt, numerical_tokens):\n",
    "    # Convert numerical_tokens to a tensor if it's a dictionary\n",
    "    if isinstance(numerical_tokens, dict):\n",
    "        # Create a mapping from token indices to actual numbers\n",
    "        token_to_number = {v: k for k, v in numerical_tokens.items()}\n",
    "        \n",
    "        # Sort tokens by their numerical values\n",
    "        sorted_tokens = sorted(token_to_number.items(), key=lambda x: float(x[1]))\n",
    "        token_indices = torch.tensor([idx for idx, _ in sorted_tokens], device=device)\n",
    "        token_to_number = dict(sorted_tokens)\n",
    "    else:\n",
    "        token_indices = torch.tensor(numerical_tokens, device=device)\n",
    "    \n",
    "    # Run model with cache to get activations at each layer\n",
    "    logits, cache = model.run_with_cache(prompt)\n",
    "    \n",
    "    # Move cache to the correct device\n",
    "    cache.to(\"cuda:0\")\n",
    "    \n",
    "    # Get accumulated residual streams for each layer with LayerNorm applied\n",
    "    accumulated_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "    \n",
    "    # Convert accumulated_resid to half precision\n",
    "    accumulated_resid = accumulated_resid.half()\n",
    "    \n",
    "    # Get probabilities for numerical tokens at each layer\n",
    "    layer_logits = einsum(\n",
    "        accumulated_resid,  # [layer, batch, pos, d_model]\n",
    "        model.W_U.to(device),  # [d_model, vocab_size]\n",
    "        \"layer batch pos d_model, d_model vocab -> layer batch pos vocab\"\n",
    "    )\n",
    "    \n",
    "    # Convert to probabilities with softmax\n",
    "    layer_probs = torch.softmax(layer_logits, dim=-1)\n",
    "    # Extract probabilities just for numerical tokens\n",
    "    number_probs = layer_probs[..., token_indices]\n",
    "    \n",
    "    # Average across batch and position dimensions\n",
    "    number_probs = number_probs.mean(dim=(1, 2))  # This will give shape [layer, num_tokens]\n",
    "    \n",
    "    return number_probs, labels, token_to_number, token_indices\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"2 + 2 = \"\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "probs, layer_labels, token_to_number, token_indices = analyze_number_probabilities(model, prompt, numerical_tokens)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(probs.cpu().numpy(),\n",
    "            xticklabels=[token_to_number[idx.item()] for idx in token_indices],\n",
    "            yticklabels=layer_labels,\n",
    "            cmap='viridis')\n",
    "plt.xlabel('Numbers')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Average Probability Distribution over Numbers Across Layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_number_probabilities(model, prompt, numerical_tokens):\n",
    "    # Convert numerical_tokens to a tensor if it's a dictionary\n",
    "    if isinstance(numerical_tokens, dict):\n",
    "        # Create a mapping from token indices to actual numbers\n",
    "        token_to_number = {v: k for k, v in numerical_tokens.items()}\n",
    "        \n",
    "        # Filter tokens to include only numbers between 0 and 10\n",
    "        filtered_tokens = {idx: num for idx, num in token_to_number.items() \n",
    "                         if num.strip().isdigit() and 0 <= float(num) <= 10}\n",
    "        \n",
    "        # Sort tokens by their numerical values\n",
    "        sorted_tokens = sorted(filtered_tokens.items(), key=lambda x: float(x[1]))\n",
    "        token_indices = torch.tensor([idx for idx, _ in sorted_tokens], device=device)\n",
    "        token_to_number = dict(sorted_tokens)\n",
    "    else:\n",
    "        token_indices = torch.tensor(numerical_tokens, device=device)\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    logits, cache = model.run_with_cache(prompt)\n",
    "    cache.to(\"cuda:0\")\n",
    "    accumulated_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "    accumulated_resid = accumulated_resid.half()\n",
    "    \n",
    "    layer_logits = einsum(\n",
    "        accumulated_resid,\n",
    "        model.W_U.to(device),\n",
    "        \"layer batch pos d_model, d_model vocab -> layer batch pos vocab\"\n",
    "    )\n",
    "    \n",
    "    layer_probs = torch.softmax(layer_logits, dim=-1)\n",
    "    number_probs = layer_probs[..., token_indices]\n",
    "    number_probs = number_probs.mean(dim=(1, 2))\n",
    "    \n",
    "    return number_probs, labels, token_to_number, token_indices\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"2 + 2 = \"\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "probs, layer_labels, token_to_number, token_indices = analyze_number_probabilities(model, prompt, numerical_tokens)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(probs.cpu().numpy(),\n",
    "            xticklabels=[token_to_number[idx.item()] for idx in token_indices],\n",
    "            yticklabels=layer_labels,\n",
    "            cmap='viridis')\n",
    "plt.xlabel('Numbers')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Average Probability Distribution over Numbers Across Layers')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent label cutoff\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_number_probabilities(model, prompt, numerical_tokens):\n",
    "    # Convert numerical_tokens to a tensor if it's a dictionary\n",
    "    if isinstance(numerical_tokens, dict):\n",
    "        # Create a mapping from token indices to actual numbers\n",
    "        token_to_number = {v: k for k, v in numerical_tokens.items()}\n",
    "        \n",
    "        # Filter tokens to include only numbers between 0 and 10\n",
    "        filtered_tokens = {idx: num for idx, num in token_to_number.items() \n",
    "                         if num.strip().isdigit() and 0 <= float(num) <= 10}\n",
    "        \n",
    "        # Sort tokens by their numerical values\n",
    "        sorted_tokens = sorted(filtered_tokens.items(), key=lambda x: float(x[1]))\n",
    "        token_indices = torch.tensor([idx for idx, _ in sorted_tokens], device=device)\n",
    "        token_to_number = dict(sorted_tokens)\n",
    "    else:\n",
    "        token_indices = torch.tensor(numerical_tokens, device=device)\n",
    "    \n",
    "    # Run model with cache to get activations at each layer\n",
    "    logits, cache = model.run_with_cache(prompt)\n",
    "    cache.to(\"cuda:0\")\n",
    "    \n",
    "    # Get residual stream at each layer\n",
    "    resid_post = torch.stack([cache[\"resid_post\", i] for i in range(model.cfg.n_layers)])\n",
    "    \n",
    "    # Apply layer norm\n",
    "    resid_post = model.ln_final(resid_post)\n",
    "    \n",
    "    # Get probabilities for numerical tokens at each layer\n",
    "    layer_logits = einsum(\n",
    "        resid_post,  # [layer, batch, pos, d_model]\n",
    "        model.W_U.to(device),  # [d_model, vocab_size]\n",
    "        \"layer batch pos d_model, d_model vocab -> layer batch pos vocab\"\n",
    "    )\n",
    "    \n",
    "    # Convert to probabilities with softmax\n",
    "    layer_probs = torch.softmax(layer_logits, dim=-1)\n",
    "    # Extract probabilities just for numerical tokens\n",
    "    number_probs = layer_probs[..., token_indices]\n",
    "    \n",
    "    # Average across batch and position dimensions\n",
    "    number_probs = number_probs.mean(dim=(1, 2))  # This will give shape [layer, num_tokens]\n",
    "    \n",
    "    # Create layer labels\n",
    "    layer_labels = [f\"Layer {i}\" for i in range(model.cfg.n_layers)]\n",
    "    \n",
    "    return number_probs, layer_labels, token_to_number, token_indices\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"2 + 2 = \"\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "probs, layer_labels, token_to_number, token_indices = analyze_number_probabilities(model, prompt, numerical_tokens)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(probs.cpu().numpy(),\n",
    "            xticklabels=[token_to_number[idx.item()] for idx in token_indices],\n",
    "            yticklabels=layer_labels,\n",
    "            cmap='viridis')\n",
    "plt.xlabel('Numbers')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Average Probability Distribution over Numbers Across Layers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_group(probs, layer_labels, token_to_number, token_indices, start_layer, end_layer, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(probs[start_layer:end_layer].cpu().numpy(),\n",
    "                xticklabels=[token_to_number[idx.item()] for idx in token_indices],\n",
    "                yticklabels=layer_labels[start_layer:end_layer],\n",
    "                cmap='viridis')\n",
    "    plt.xlabel('Numbers')\n",
    "    plt.ylabel('Layer')\n",
    "    plt.title(f'Probability Distribution over Numbers - Layers {start_layer}-{end_layer-1} {title_suffix}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get the probabilities as before\n",
    "prompt = \"2 + 2 = \"\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "probs, layer_labels, token_to_number, token_indices = analyze_number_probabilities(model, prompt, numerical_tokens)\n",
    "\n",
    "# Plot in groups of 8 layers\n",
    "for i in range(0, 32, 1):\n",
    "    plot_layer_group(probs, layer_labels, token_to_number, token_indices, i, i+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
