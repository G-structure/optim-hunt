{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "from optim_hunter.plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference, line\n",
    "from optim_hunter.utils import prepare_prompt, slice_dataset\n",
    "from optim_hunter.sklearn_regressors import linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random\n",
    "from optim_hunter.datasets import get_dataset_friedman_2\n",
    "from optim_hunter.data_model import create_comparison_data\n",
    "from optim_hunter.model_utils import get_numerical_tokens, generate_linreg_tokens, run_and_cache_model_linreg_tokens_batched, run_and_cache_model_linreg_tokens\n",
    "from optim_hunter.llama_model import load_llama_model\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "#device = t.device(\"cuda:0,1\" if t.cuda.is_available() else \"cpu\")\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# device = t.device(\"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3785158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61bd26fa2b4e39a1d89d85cf770c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = load_llama_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d667ec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numerical tokens: {'782': 23833, '874': 25822, '553': 22663, '778': 23592, '801': 17973, '556': 20866, '912': 22750, '700': 7007, '541': 22058, '872': 25303, '939': 26164, '999': 5500, '511': 18625, '768': 17521, '020': 11139, '101': 4645, '388': 19081, '23': 1419, '656': 20744, '798': 26519, '584': 23816, '05': 2304, '437': 18318, '790': 22876, '342': 17590, '489': 22418, '941': 21322, '630': 18660, '681': 25091, '879': 25622, '5': 20, '881': 25339, '901': 19319, '858': 23805, '044': 20078, '444': 14870, '290': 13754, '687': 21897, '786': 25251, '248': 14185, '663': 24491, '658': 23654, '146': 10465, '675': 21129, '877': 23873, '075': 22679, '272': 15741, '632': 20775, '045': 23785, '676': 24187, '063': 24254, '586': 22345, '929': 25344, '770': 20772, '472': 21757, '177': 11242, '0': 15, '947': 26511, '329': 18196, '774': 24472, '233': 12994, '746': 25594, '910': 21056, '005': 8504, '389': 20422, '864': 19355, '63': 5495, '97': 3534, '225': 11057, '087': 27311, '900': 7467, '549': 22782, '046': 24222, '406': 17264, '268': 16332, '615': 21385, '209': 12652, '847': 25125, '383': 19230, '891': 24962, '245': 13078, '018': 16745, '814': 25498, '269': 16955, '800': 4728, '096': 28153, '40': 1272, '77': 2813, '583': 23493, '062': 19222, '153': 9800, '068': 26661, '259': 15537, '293': 17313, '381': 19162, '416': 17763, '619': 23388, '097': 28384, '767': 23275, '69': 3076, '587': 22159, '318': 17592, '361': 18277, '94': 6281, '968': 23386, '861': 24963, '081': 22534, '896': 24542, '684': 24313, '595': 22754, '095': 26421, '377': 10898, '131': 9263, '889': 25354, '385': 18695, '399': 18572, '031': 18887, '113': 8190, '885': 19445, '976': 25208, '50': 1135, '18': 972, '793': 24531, '100': 1041, '160': 6330, '034': 22379, '180': 5245, '334': 17153, '518': 21312, '762': 24376, '742': 25560, '917': 24391, '353': 17228, '815': 23582, '366': 18044, '277': 16367, '934': 24347, '420': 12819, '824': 25016, '688': 23292, '202': 2366, '922': 20275, '735': 24939, '166': 11247, '542': 21791, '745': 23901, '945': 24599, '723': 24388, '647': 22644, '064': 20478, '779': 25659, '506': 19673, '368': 19057, '67': 3080, '370': 14648, '959': 26328, '298': 17690, '734': 24438, '799': 23987, '714': 23193, '215': 12112, '899': 22889, '240': 8273, '753': 25504, '936': 25612, '645': 22926, '112': 7261, '706': 22457, '27': 1544, '641': 23525, '26': 1627, '907': 23505, '345': 12901, '300': 3101, '163': 9892, '574': 23402, '475': 19799, '805': 21032, '365': 12676, '436': 21299, '761': 25110, '230': 9870, '282': 16544, '461': 19608, '251': 13860, '802': 13135, '356': 18349, '156': 10132, '588': 20691, '661': 24132, '580': 18216, '264': 12815, '32': 843, '165': 10680, '614': 22638, '780': 19423, '346': 18061, '537': 19038, '819': 18831, '083': 25077, '677': 24375, '276': 16660, '09': 2545, '777': 15831, '822': 23105, '159': 11068, '607': 21996, '181': 10562, '812': 19270, '206': 11056, '755': 23532, '977': 26409, '337': 17609, '623': 22801, '142': 10239, '773': 23267, '030': 14649, '415': 18136, '703': 20436, '608': 19944, '633': 23736, '648': 23802, '522': 20936, '014': 15901, '610': 17608, '860': 18670, '612': 21018, '620': 17416, '134': 9565, '649': 24734, '002': 6726, '007': 11194, '022': 18642, '718': 21982, '967': 27134, '335': 16596, '135': 8878, '432': 16739, '635': 22276, '931': 25717, '43': 3391, '697': 25388, '969': 24792, '662': 24199, '210': 8848, '689': 25458, '16': 845, '993': 24242, '292': 16443, '873': 25747, '390': 15515, '866': 22455, '701': 19597, '589': 22905, '666': 10943, '179': 11128, '418': 19770, '705': 21469, '581': 23864, '850': 16217, '919': 24337, '208': 12171, '321': 14423, '29': 1682, '405': 16408, '516': 20571, '743': 26260, '558': 22895, '060': 15101, '073': 25779, '242': 12754, '167': 11515, '84': 5833, '932': 25401, '604': 20354, '30': 966, '636': 22422, '784': 22148, '236': 14087, '079': 27407, '143': 10290, '653': 21598, '971': 25693, '44': 2096, '638': 24495, '454': 20555, '203': 9639, '640': 14033, '325': 15257, '162': 10674, '596': 24515, '155': 9992, '898': 24809, '376': 18322, '973': 24471, '374': 18265, '11': 806, '533': 21876, '682': 25178, '707': 18770, '343': 16522, '152': 9756, '504': 18048, '686': 22347, '307': 14777, '876': 24870, '077': 23102, '01': 1721, '74': 5728, '284': 17058, '278': 16949, '590': 20615, '995': 22101, '229': 14378, '038': 24462, '338': 18633, '865': 24678, '157': 10895, '291': 17335, '35': 1758, '937': 21936, '38': 1987, '751': 23986, '878': 25890, '480': 11738, '118': 8899, '333': 8765, '884': 25962, '978': 17272, '740': 21112, '071': 24508, '122': 8259, '503': 17735, '966': 25285, '302': 13121, '281': 15282, '616': 21379, '033': 13103, '384': 12910, '509': 12448, '507': 20068, '609': 21138, '868': 25862, '355': 17306, '069': 27325, '224': 10697, '52': 4103, '228': 14261, '722': 23024, '605': 19666, '220': 8610, '495': 21038, '358': 17112, '28': 1591, '524': 21177, '250': 5154, '266': 15999, '979': 25476, '407': 18501, '238': 13895, '104': 6849, '758': 25302, '301': 12405, '894': 26227, '3': 18, '961': 26114, '015': 16037, '049': 25307, '260': 11387, '286': 17361, '467': 20419, '008': 11436, '997': 22694, '796': 24832, '75': 2075, '367': 18775, '108': 6640, '731': 24626, '646': 22642, '048': 23904, '942': 20249, '352': 16482, '327': 13817, '056': 25921, '344': 17451, '398': 19838, '593': 22608, '339': 17887, '892': 24110, '200': 1049, '304': 12166, '98': 3264, '55': 2131, '787': 23171, '808': 11770, '216': 12463, '776': 23823, '363': 18199, '76': 4767, '349': 18634, '551': 21860, '065': 26478, '597': 24574, '466': 21404, '14': 975, '671': 23403, '539': 23033, '904': 22777, '710': 19027, '331': 16707, '41': 3174, '840': 19899, '830': 21221, '716': 23929, '536': 21600, '712': 22708, '79': 4643, '07': 2589, '445': 19697, '791': 26234, '970': 21133, '51': 3971, '702': 20253, '965': 24837, '182': 10828, '849': 26537, '303': 13236, '914': 24579, '502': 17824, '22': 1313, '373': 18017, '698': 25169, '974': 26007, '651': 23409, '513': 21164, '91': 5925, '421': 18245, '717': 25150, '296': 17408, '64': 1227, '169': 11739, '727': 23486, '137': 10148, '980': 19068, '952': 24597, '986': 27468, '781': 21893, '821': 23282, '680': 17814, '186': 9714, '775': 22908, '598': 21856, '375': 12935, '708': 21295, '354': 18384, '036': 23110, '557': 23906, '456': 10961, '411': 17337, '423': 19711, '625': 15894, '280': 11209, '409': 12378, '103': 6889, '123': 4513, '003': 6268, '119': 9079, '806': 22397, '223': 12533, '950': 15862, '783': 26008, '48': 2166, '070': 17819, '322': 15805, '234': 11727, '184': 10336, '001': 4119, '413': 19288, '328': 16884, '471': 20617, '9': 24, '87': 4044, '459': 22094, '527': 22369, '996': 23031, '80': 1490, '771': 24876, '827': 24920, '843': 23996, '611': 20973, '869': 26497, '563': 21789, '988': 24538, '883': 24902, '013': 16368, '295': 16780, '654': 21969, '12': 717, '185': 9741, '039': 21602, '728': 24054, '683': 24887, '098': 26983, '193': 7285, '422': 16460, '262': 14274, '196': 5162, '314': 16104, '92': 6083, '15': 868, '737': 22039, '606': 20213, '449': 21125, '139': 10125, '532': 20711, '809': 21474, '994': 22897, '254': 12375, '895': 25238, '903': 23305, '826': 23038, '434': 20165, '055': 22913, '924': 23890, '372': 17662, '453': 20235, '323': 15726, '021': 11592, '627': 23103, '227': 14206, '6': 21, '255': 3192, '591': 24380, '351': 18113, '813': 24288, '951': 24989, '639': 23079, '916': 24487, '750': 11711, '95': 2721, '56': 3487, '168': 8953, '439': 20963, '332': 17079, '440': 14868, '71': 6028, '905': 22393, '217': 13460, '88': 2421, '316': 15340, '172': 10861, '341': 16546, '923': 22614, '111': 5037, '417': 19561, '577': 23411, '748': 20338, '804': 20417, '000': 931, '089': 25867, '330': 10568, '040': 12505, '435': 19305, '713': 22977, '174': 11771, '690': 21741, '433': 20153, '561': 20460, '724': 24735, '438': 20596, '870': 22440, '054': 25230, '285': 15935, '357': 18520, '915': 22387, '624': 23000, '749': 25541, '880': 19272, '161': 10718, '116': 8027, '404': 7507, '235': 12422, '825': 22091, '211': 11483, '875': 17419, '926': 26026, '81': 5932, '360': 6843, '362': 18509, '170': 8258, '309': 15500, '958': 27079, '766': 25358, '239': 14815, '956': 26067, '757': 23776, '494': 22054, '481': 21235, '31': 2148, '189': 9378, '270': 10914, '962': 26366, '197': 4468, '853': 25724, '261': 15602, '072': 23439, '795': 25808, '572': 22468, '244': 13719, '854': 25515, '403': 13074, '837': 26244, '401': 10841, '247': 14125, '491': 21824, '552': 21478, '458': 21209, '601': 18262, '86': 4218, '294': 17168, '299': 15531, '855': 22869, '991': 24606, '78': 2495, '214': 11584, '446': 20385, '569': 23642, '498': 21962, '124': 8874, '987': 22207, '672': 22768, '080': 13837, '833': 22904, '336': 17014, '313': 15231, '380': 13897, '657': 23480, '930': 19306, '704': 21949, '559': 22424, '729': 22194, '567': 19282, '89': 4578, '679': 25136, '935': 26970, '938': 25454, '886': 25399, '699': 23459, '492': 21776, '7': 22, '133': 9423, '603': 21006, '68': 2614, '797': 25314, '62': 5538, '565': 20943, '817': 25528, '47': 2618, '364': 15951, '911': 17000, '828': 22716, '964': 26281, '585': 21535, '621': 22488, '57': 3226, '72': 5332, '067': 27309, '520': 15830, '836': 25192, '571': 22005, '473': 21505, '933': 25806, '685': 23717, '496': 19447, '643': 22956, '859': 24061, '082': 24996, '043': 17776, '634': 24307, '957': 27341, '887': 26058, '132': 9413, '803': 20899, '949': 24680, '747': 23619, '198': 3753, '195': 6280, '028': 22000, '810': 19232, '816': 23713, '960': 16415, '190': 7028, '759': 26439, '451': 20360, '570': 18712, '560': 17698, '925': 21910, '592': 20128, '178': 11256, '460': 16551, '83': 6069, '732': 24289, '419': 19391, '842': 25377, '050': 16193, '871': 25665, '66': 2287, '525': 18415, '396': 19615, '548': 22287, '397': 20698, '566': 23477, '243': 14052, '006': 11030, '754': 23952, '918': 25828, '834': 26223, '183': 10750, '49': 2491, '665': 23467, '010': 7755, '477': 21144, '59': 2946, '115': 7322, '148': 10410, '637': 21788, '848': 24951, '219': 13762, '921': 22536, '721': 20873, '526': 22593, '543': 19642, '312': 13384, '93': 6365, '96': 4161, '818': 23141, '20': 508, '462': 20911, '838': 24250, '117': 8546, '253': 14022, '529': 21618, '412': 17574, '102': 4278, '265': 14374, '893': 26088, '476': 22191, '263': 15666, '129': 9748, '429': 16371, '70': 2031, '052': 24130, '279': 17267, '829': 26218, '989': 25350, '222': 9716, '10': 605, '387': 20062, '175': 10005, '470': 17711, '486': 21511, '888': 12251, '442': 20502, '08': 2318, '032': 21040, '099': 26513, '981': 25643, '241': 13341, '531': 20823, '835': 23424, '061': 23324, '371': 18650, '130': 5894, '642': 22266, '650': 13655, '035': 22407, '90': 1954, '463': 21290, '140': 6860, '831': 25009, '428': 19140, '469': 21330, '105': 6550, '126': 9390, '078': 24850, '613': 22922, '04': 2371, '231': 12245, '908': 23629, '765': 22240, '629': 24239, '913': 24331, '998': 19416, '074': 26739, '02': 2437, '882': 23213, '047': 24970, '13': 1032, '65': 2397, '448': 19956, '145': 9591, '628': 23574, '863': 26051, '902': 21026, '066': 23835, '057': 26866, '023': 20063, '427': 20465, '187': 9674, '8': 23, '465': 19988, '741': 25021, '846': 26563, '107': 7699, '315': 15189, '147': 10288, '602': 20224, '25': 914, '173': 11908, '149': 10161, '019': 18089, '258': 15966, '555': 14148, '674': 25513, '954': 25741, '188': 9367, '2': 17, '733': 24865, '711': 22375, '538': 22600, '497': 22640, '582': 23670, '150': 3965, '016': 15794, '726': 24430, '906': 22224, '090': 18807, '983': 24742, '867': 26013, '273': 15451, '545': 20749, '61': 5547, '093': 25202, '664': 23888, '488': 21310, '508': 19869, '011': 10731, '34': 1958, '110': 5120, '271': 15828, '221': 12425, '644': 21975, '54': 4370, '094': 26195, '622': 19808, '540': 17048, '785': 22539, '82': 6086, '856': 25505, '928': 25001, '267': 16567, '424': 18517, '084': 26720, '306': 12879, '490': 18518, '45': 1774, '514': 20998, '897': 24777, '736': 23969, '274': 16590, '426': 20363, '144': 8929, '394': 20077, '305': 13364, '447': 20800, '199': 2550, '226': 14057, '19': 777, '599': 21944, '395': 19498, '550': 13506, '382': 18781, '1': 16, '483': 21884, '391': 19631, '953': 25326, '578': 22915, '192': 5926, '695': 24394, '443': 17147, '678': 17458, '289': 17212, '478': 22086, '114': 8011, '626': 22385, '660': 19274, '851': 24866, '692': 25073, '659': 25090, '600': 5067, '151': 9690, '275': 14417, '191': 7529, '474': 21358, '975': 24609, '246': 14205, '579': 24847, '176': 10967, '088': 25620, '194': 6393, '890': 21381, '053': 25210, '400': 3443, '670': 21218, '310': 12226, '213': 11702, '696': 23578, '33': 1644, '927': 26437, '617': 21717, '138': 10350, '348': 19746, '256': 4146, '862': 24071, '058': 24824, '212': 11227, '324': 16723, '521': 20767, '844': 24344, '204': 7854, '568': 22049, '457': 21675, '106': 7461, '85': 5313, '839': 25465, '669': 25289, '772': 23624, '852': 24571, '205': 10866, '946': 26491, '468': 20304, '573': 22529, '06': 2705, '37': 1806, '763': 26083, '136': 9795, '564': 22210, '207': 12060, '528': 21458, '515': 19633, '326': 17470, '631': 21729, '99': 1484, '499': 18162, '154': 10559, '39': 2137, '311': 15134, '121': 7994, '452': 21098, '410': 14487, '972': 24425, '059': 27033, '414': 17448, '085': 24646, '982': 25873, '788': 24216, '715': 22744, '386': 16481, '547': 23215, '694': 25392, '944': 25687, '756': 24456, '500': 2636, '594': 23428, '164': 10513, '379': 19867, '719': 24758, '464': 21033, '201': 679, '347': 17678, '393': 18252, '076': 26247, '218': 13302, '909': 21278, '985': 24961, '963': 26087, '546': 22048, '576': 20758, '845': 24650, '431': 19852, '319': 16874, '823': 23848, '125': 6549, '171': 11123, '232': 12338, '252': 12326, '158': 11286, '03': 2839, '807': 23178, '128': 4386, '430': 14245, '53': 4331, '510': 15633, '237': 14590, '4': 19, '487': 22184, '009': 13858, '482': 21984, '73': 5958, '992': 22992, '58': 2970, '544': 21239, '841': 25496, '249': 14735, '485': 19773, '512': 8358, '024': 19592, '730': 20785, '673': 24938, '792': 24763, '720': 13104, '760': 19104, '693': 25298, '725': 23309, '990': 19146, '948': 26979, '402': 16496, '24': 1187, '857': 20907, '484': 20339, '042': 22349, '450': 10617, '794': 25926, '297': 18163, '943': 26576, '752': 23644, '534': 22467, '350': 8652, '141': 9335, '012': 11531, '359': 19192, '832': 23879, '479': 21848, '667': 19774, '955': 25875, '575': 21228, '493': 22741, '21': 1691, '037': 23587, '026': 21641, '554': 22303, '42': 2983, '691': 25168, '340': 13679, '535': 20618, '505': 17786, '562': 19242, '025': 18070, '051': 23545, '60': 1399, '378': 19166, '283': 16085, '092': 25221, '655': 15573, '029': 23273, '00': 410, '320': 9588, '308': 14498, '425': 17837, '287': 17897, '317': 16718, '041': 20945, '811': 22588, '618': 21985, '920': 18485, '455': 20325, '004': 8759, '769': 24619, '820': 18248, '091': 24443, '652': 23181, '086': 26956, '789': 16474, '739': 25809, '940': 21251, '668': 24427, '127': 6804, '46': 2790, '027': 21360, '738': 25527, '288': 15287, '257': 15574, '709': 22874, '120': 4364, '17': 1114, '764': 24402, '109': 7743, '984': 23812, '408': 18058, '530': 17252, '36': 1927, '441': 18495, '392': 19695, '017': 17248, '523': 21123, '501': 14408, '744': 23800, '369': 19929, '519': 21851, '517': 22507}\n",
      "Number of numerical tokens: 1110\n",
      "Zero token present: True\n",
      "Token ID for zero: 1682\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "print(\"Found numerical tokens:\", numerical_tokens)\n",
    "print(\"Number of numerical tokens:\", len(numerical_tokens))\n",
    "\n",
    "\n",
    "# Check if digit 0 is in numerical tokens\n",
    "has_zero = '0' in numerical_tokens\n",
    "print(\"Zero token present:\", has_zero)\n",
    "if has_zero:\n",
    "    print(\"Token ID for zero:\", numerical_tokens['29'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f7effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(linreg_tokens, linreg_logits, linreg_caches, linreg_data_store) = run_and_cache_model_linreg_tokens_batched(\n",
    "    model,\n",
    "    seq_len=25,\n",
    "    total_batch=25\n",
    ")\n",
    "\n",
    "model.clear_contexts()\n",
    "\n",
    "# Move all tokens and logits to CPU\n",
    "linreg_tokens = [tokens.to('cpu') for tokens in linreg_tokens]\n",
    "linreg_logits = [logits.to('cpu') for logits in linreg_logits]\n",
    "# Caches are already on CPU from the batched function\n",
    "\n",
    "# Verify all datasets have the same comparison names\n",
    "base_comparison_names = linreg_data_store[0][\"comparison_names\"]\n",
    "all_match = all(dataset[\"comparison_names\"] == base_comparison_names for dataset in linreg_data_store[1:])\n",
    "assert all_match, \"Mismatch in comparison names across datasets.\"\n",
    "\n",
    "# Extract comparison names from the first dataset\n",
    "token_pairs_names = base_comparison_names.copy()\n",
    "\n",
    "# Extract token pairs across all datasets for each comparison\n",
    "token_pairs = [\n",
    "    t.stack([dataset[\"token_pairs\"][i] for dataset in linreg_data_store])[0]\n",
    "    for i in range(len(token_pairs_names))\n",
    "]\n",
    "\n",
    "logger.info(f\"Number of comparisons: {len(token_pairs_names)}\")\n",
    "logger.info(f\"Number of token_pairs: {len(token_pairs)}\")\n",
    "\n",
    "# Iterate over token pairs and generate plots\n",
    "for i, token_pair in enumerate(token_pairs):\n",
    "    logger.info(f\"Processing comparison {i}: {token_pairs_names[i]}\")\n",
    "    token_pair = token_pair.to('cpu')\n",
    "\n",
    "    # Print token pair shape for debugging\n",
    "    print(f\"Token pair shape: {token_pair.shape}\")\n",
    "\n",
    "    def logits_to_ave_logit_diff(\n",
    "        logits_list: List[Float[Tensor, \"batch seq d_vocab\"]],\n",
    "        answer_tokens: Float[Tensor, \"batch 2\"] = token_pair,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        # Process each batch separately\n",
    "        all_logit_diffs = []\n",
    "        \n",
    "        for logits in logits_list:\n",
    "            final_logits = logits[:, -1, :]  # Take final position from each batch\n",
    "            \n",
    "            correct = answer_tokens[:, 0]\n",
    "            incorrect = answer_tokens[:, 1]\n",
    "\n",
    "            correct_logits = final_logits[t.arange(final_logits.size(0)), correct]\n",
    "            incorrect_logits = final_logits[t.arange(final_logits.size(0)), incorrect]\n",
    "\n",
    "            logit_diff = correct_logits - incorrect_logits\n",
    "            all_logit_diffs.append(logit_diff)\n",
    "        \n",
    "        # Combine results\n",
    "        combined_logit_diffs = t.cat(all_logit_diffs)\n",
    "        \n",
    "        if per_prompt:\n",
    "            return combined_logit_diffs\n",
    "        else:\n",
    "            return combined_logit_diffs.mean()\n",
    "\n",
    "    original_per_prompt_diff = logits_to_ave_logit_diff(linreg_logits, token_pair, per_prompt=True)\n",
    "    original_average_logit_diff = logits_to_ave_logit_diff(linreg_logits, token_pair)\n",
    "\n",
    "    # Move specific cache to GPU for processing\n",
    "    # current_cache = linreg_caches[0].to('cuda')  # Process first cache as example\n",
    "    current_cache = linreg_caches[i]\n",
    "\n",
    "    # Retrieve final residual stream\n",
    "    final_residual_stream: Float[Tensor, \"batch seq d_model\"] = current_cache[\"resid_post\", -1]\n",
    "    final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "    # Compute residual directions\n",
    "    pair_residual_directions = model.tokens_to_residual_directions(token_pair.to('cpu'))\n",
    "\n",
    "    correct_residual_directions, incorrect_residual_directions = pair_residual_directions.unbind(dim=1)\n",
    "    logit_diff_directions = correct_residual_directions - incorrect_residual_directions\n",
    "\n",
    "    def residual_stack_to_logit_diff(\n",
    "        residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "        cache: ActivationCache,\n",
    "        logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    "    ) -> Float[Tensor, \"...\"]:\n",
    "        scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "        logit_diff_directions = logit_diff_directions.to(dtype=scaled_residual_stream.dtype).to('cpu')\n",
    "\n",
    "        batch_size = residual_stack.size(-2)\n",
    "        avg_logit_diff = einops.einsum(\n",
    "            scaled_residual_stream,\n",
    "            logit_diff_directions,\n",
    "            \"... batch d_model, batch d_model -> ...\"\n",
    "        ) / batch_size\n",
    "        return avg_logit_diff\n",
    "\n",
    "    # Verify residual stack computation\n",
    "    t.testing.assert_close(\n",
    "        residual_stack_to_logit_diff(final_token_residual_stream.to(t.float32), current_cache.to(t.float32)),\n",
    "        original_average_logit_diff.to(t.float32),\n",
    "        rtol=5e-3,\n",
    "        atol=5e-3\n",
    "    )\n",
    "\n",
    "    # Accumulate residuals\n",
    "    accumulated_residual, labels = current_cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "    logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, current_cache).half()\n",
    "\n",
    "    per_layer_residual, labels = current_cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "    per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, current_cache)\n",
    "\n",
    "    # Generate plots...\n",
    "    line(\n",
    "        logit_lens_logit_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    # Move cache back to CPU and clear GPU memory\n",
    "    # current_cache = current_cache.to('cpu')\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    line(\n",
    "        per_layer_logit_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Per Layer Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "    # model = model.to(\"cpu\")\n",
    "    # linreg_cache = linreg_cache.to(\"cpu\")\n",
    "    # per_head_residual, labels = linreg_cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "    # per_head_residual = per_head_residual.to(\"cpu\")\n",
    "    # per_head_residual = einops.rearrange(\n",
    "    #     per_head_residual,\n",
    "    #     \"(layer head) ... -> layer head ...\",\n",
    "    #     layer=model.cfg.n_layers\n",
    "    # )\n",
    "    # per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, linreg_cache)\n",
    "\n",
    "    # imshow(\n",
    "    #     per_head_logit_diffs,\n",
    "    #     labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "    #     title=\"Logit Difference From Each Head\",\n",
    "    #     width=600\n",
    "    # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9630716",
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    t.cuda.empty_cache()\n",
    "# Example usage\n",
    "(linreg_tokens, linreg_logits, linreg_caches, linreg_data_store) = run_and_cache_model_linreg_tokens_batched(\n",
    "    model,\n",
    "    seq_len=25,\n",
    "    total_batch=100\n",
    ")\n",
    "\n",
    "model.clear_contexts()\n",
    "\n",
    "# Move all tokens and logits to CPU\n",
    "linreg_tokens = [tokens.to('cpu') for tokens in linreg_tokens]\n",
    "linreg_logits = [logits.to('cpu') for logits in linreg_logits]\n",
    "\n",
    "# Verify all datasets have the same comparison names\n",
    "base_comparison_names = linreg_data_store[0][\"comparison_names\"]\n",
    "all_match = all(dataset[\"comparison_names\"] == base_comparison_names for dataset in linreg_data_store[1:])\n",
    "assert all_match, \"Mismatch in comparison names across datasets.\"\n",
    "\n",
    "# Extract comparison names from the first dataset\n",
    "token_pairs_names = base_comparison_names.copy()\n",
    "\n",
    "# Extract token pairs across all datasets for each comparison\n",
    "token_pairs = [\n",
    "    t.stack([dataset[\"token_pairs\"][i] for dataset in linreg_data_store])[0]\n",
    "    for i in range(len(token_pairs_names))\n",
    "]\n",
    "\n",
    "logger.info(f\"Number of comparisons: {len(token_pairs_names)}\")\n",
    "logger.info(f\"Number of token_pairs: {len(token_pairs)}\")\n",
    "\n",
    "# Iterate over token pairs and generate plots\n",
    "for i, token_pair in enumerate(token_pairs):\n",
    "    logger.info(f\"Processing comparison {i}: {token_pairs_names[i]}\")\n",
    "    token_pair = token_pair.to('cpu')\n",
    "\n",
    "    print(f\"Token pair shape: {token_pair.shape}\")\n",
    "\n",
    "    def logits_to_ave_logit_diff(\n",
    "        logits_list: List[Float[Tensor, \"batch seq d_vocab\"]],\n",
    "        answer_tokens: Float[Tensor, \"batch 2\"] = token_pair,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        # Process each batch separately\n",
    "        all_logit_diffs = []\n",
    "        \n",
    "        for logits in logits_list:\n",
    "            final_logits = logits[:, -1, :]  # Take final position from each batch\n",
    "            \n",
    "            correct = answer_tokens[:, 0]\n",
    "            incorrect = answer_tokens[:, 1]\n",
    "\n",
    "            correct_logits = final_logits[t.arange(final_logits.size(0)), correct]\n",
    "            incorrect_logits = final_logits[t.arange(final_logits.size(0)), incorrect]\n",
    "\n",
    "            logit_diff = correct_logits - incorrect_logits\n",
    "            all_logit_diffs.append(logit_diff)\n",
    "        \n",
    "        # Combine results\n",
    "        combined_logit_diffs = t.cat(all_logit_diffs)\n",
    "        \n",
    "        if per_prompt:\n",
    "            return combined_logit_diffs\n",
    "        else:\n",
    "            return combined_logit_diffs.mean()\n",
    "\n",
    "    original_per_prompt_diff = logits_to_ave_logit_diff(linreg_logits, token_pair, per_prompt=True)\n",
    "    original_average_logit_diff = logits_to_ave_logit_diff(linreg_logits, token_pair)\n",
    "\n",
    "    # Initialize lists to store results from all caches\n",
    "    all_logit_lens_diffs = []\n",
    "    all_per_layer_diffs = []\n",
    "    all_per_head_diffs = []\n",
    "\n",
    "    # Process each cache\n",
    "    for cache_idx, current_cache in enumerate(linreg_caches):\n",
    "        logger.info(f\"Processing cache {cache_idx}\")\n",
    "        \n",
    "        # Move cache to GPU\n",
    "        current_cache = current_cache\n",
    "\n",
    "        # Retrieve final residual stream\n",
    "        final_residual_stream = current_cache[\"resid_post\", -1]\n",
    "        final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "\n",
    "        # Compute residual directions\n",
    "        pair_residual_directions = model.tokens_to_residual_directions(token_pair.to('cpu'))\n",
    "        correct_residual_directions, incorrect_residual_directions = pair_residual_directions.unbind(dim=1)\n",
    "        logit_diff_directions = correct_residual_directions - incorrect_residual_directions\n",
    "\n",
    "        def residual_stack_to_logit_diff(\n",
    "            residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "            cache: ActivationCache,\n",
    "            logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    "        ) -> Float[Tensor, \"...\"]:\n",
    "            scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "            logit_diff_directions = logit_diff_directions.to(dtype=scaled_residual_stream.dtype).to('cpu')\n",
    "\n",
    "            batch_size = residual_stack.size(-2)\n",
    "            avg_logit_diff = einops.einsum(\n",
    "                scaled_residual_stream,\n",
    "                logit_diff_directions,\n",
    "                \"... batch d_model, batch d_model -> ...\"\n",
    "            ) / batch_size\n",
    "            return avg_logit_diff\n",
    "\n",
    "        # Accumulate residuals\n",
    "        accumulated_residual, labels = current_cache.accumulated_resid(\n",
    "            layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    "        )\n",
    "        logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, current_cache).half()\n",
    "        all_logit_lens_diffs.append(logit_lens_logit_diffs)\n",
    "\n",
    "        # Per layer analysis\n",
    "        per_layer_residual, _ = current_cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "        per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, current_cache)\n",
    "        all_per_layer_diffs.append(per_layer_logit_diffs)\n",
    "\n",
    "\n",
    "        # model = model.to('cpu')\n",
    "        # if t.cuda.is_available():\n",
    "        #     t.cuda.empty_cache()\n",
    "\n",
    "        # # Per head analysis\n",
    "        # model = model.to(\"cpu\")\n",
    "        # current_cache = current_cache.to(\"cuda:1\")\n",
    "        # per_head_residual, _ = current_cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "        # per_head_residual = per_head_residual.to(\"cuda\")\n",
    "        # per_head_residual = einops.rearrange(\n",
    "        #     per_head_residual,\n",
    "        #     \"(layer head) ... -> layer head ...\",\n",
    "        #     layer=model.cfg.n_layers\n",
    "        # )\n",
    "        # per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, current_cache)\n",
    "        # all_per_head_diffs.append(per_head_logit_diffs)\n",
    "\n",
    "        # Clear GPU memory\n",
    "        current_cache = current_cache.to('cpu')\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.empty_cache()\n",
    "\n",
    "    # Average results across all caches\n",
    "    avg_logit_lens_diffs = t.stack(all_logit_lens_diffs).mean(dim=0)\n",
    "    avg_per_layer_diffs = t.stack(all_per_layer_diffs).mean(dim=0)\n",
    "    # avg_per_head_diffs = t.stack(all_per_head_diffs).mean(dim=0)\n",
    "\n",
    "    # Generate plots with averaged results\n",
    "    line(\n",
    "        avg_logit_lens_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Average Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    line(\n",
    "        avg_per_layer_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Average Per Layer Logit Difference for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    # imshow(\n",
    "    #     avg_per_head_diffs,\n",
    "    #     labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    #     title=f\"Average Logit Difference From Each Head for {token_pairs_names[i]}\",\n",
    "    #     width=600\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73a785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab0b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "model.clear_contexts()\n",
    "\n",
    "# Extract predictions across all datasets and convert to tensors\n",
    "predictions_store = {\n",
    "    predictor: t.tensor([\n",
    "        dataset[\"predictions\"][predictor] \n",
    "        for dataset in linreg_data_store\n",
    "    ], dtype=t.float32)\n",
    "    for predictor in linreg_data_store[0][\"predictions\"].keys()\n",
    "}\n",
    "\n",
    "# Print predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(\"-\" * 50)\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\n{predictor_name}:\")\n",
    "    for i, pred in enumerate(predictor_tensor):\n",
    "        print(f\"  Sample {i}: {pred:.2f}\")\n",
    "\n",
    "# Process each predictor\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\nPredictor: {predictor_name}\")\n",
    "    print(f\"Shape: {predictor_tensor.shape}\")\n",
    "    print(f\"Values: {predictor_tensor}\")\n",
    "\n",
    "    numerical_token_ids = t.tensor(list(numerical_tokens.values()), device=\"cpu\").expand(predictor_tensor.shape[0], -1)\n",
    "    print(f\"Numerical token IDs shape: {numerical_token_ids.shape}\")\n",
    "\n",
    "\n",
    "    def logits_to_numeric_mse(\n",
    "        logits_list: List[Float[Tensor, \"batch seq d_vocab\"]],\n",
    "        predicted_values: Float[Tensor, \"batch\"],\n",
    "        numerical_tokens: dict,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        '''\n",
    "        Handles logits with potentially different sequence lengths\n",
    "        '''\n",
    "        # Get final token logits from each batch\n",
    "        final_logits_list = [logits[:, -1, :] for logits in logits_list]\n",
    "        # Combine final logits\n",
    "        combined_final_logits = t.cat(final_logits_list, dim=0)\n",
    "        \n",
    "        numeric_ids = []\n",
    "        numeric_values = []\n",
    "        for digit, token_id in numerical_tokens.items():\n",
    "            try:\n",
    "                value = float(digit)\n",
    "                numeric_ids.append(token_id)\n",
    "                numeric_values.append(value)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        numeric_ids = t.tensor(numeric_ids, device=combined_final_logits.device)\n",
    "        numeric_values = t.tensor(numeric_values, device=combined_final_logits.device)\n",
    "        \n",
    "        numeric_logits = combined_final_logits[:, numeric_ids]\n",
    "        numeric_probs = t.softmax(numeric_logits, dim=-1)\n",
    "        \n",
    "        expected_values = (numeric_probs * numeric_values.unsqueeze(0)).sum(dim=-1)\n",
    "        \n",
    "        mse = (expected_values - predicted_values) ** 2\n",
    "        \n",
    "        if per_prompt:\n",
    "            return mse\n",
    "        else:\n",
    "            return mse.mean()\n",
    "\n",
    "\n",
    "    original_per_prompt_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens, per_prompt=True)\n",
    "    original_average_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens)\n",
    "\n",
    "    # Initialize lists to store results from all caches\n",
    "    all_layerwise_mse = []\n",
    "    all_expected_values = []\n",
    "\n",
    "    # Process each cache\n",
    "    for cache_idx, current_cache in enumerate(linreg_caches):\n",
    "        logger.info(f\"Processing cache {cache_idx}\")\n",
    "        \n",
    "        # Move cache to GPU\n",
    "        # current_cache = current_cache.to('cuda')\n",
    "\n",
    "        # Compute residual directions for numeric MSE\n",
    "        numeric_residual_directions = model.tokens_to_residual_directions(numerical_token_ids).to(\"cpu\")\n",
    "\n",
    "        def get_numeric_logits_per_layer(\n",
    "            residual_stack: Float[Tensor, \"layers batch d_model\"],\n",
    "            cache: ActivationCache,\n",
    "            numeric_residual_directions: Float[Tensor, \"batch num_tokens d_model\"]\n",
    "        ) -> Float[Tensor, \"layers batch num_tokens\"]:\n",
    "            scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "            scaled_residual_stream = scaled_residual_stream.to(\"cpu\")\n",
    "            numeric_residual_directions = numeric_residual_directions.to(dtype=scaled_residual_stream.dtype)\n",
    "\n",
    "            numeric_logits = einops.einsum(\n",
    "                scaled_residual_stream,\n",
    "                numeric_residual_directions,\n",
    "                \"layer b d, b t d -> layer b t\"\n",
    "            )\n",
    "            return numeric_logits\n",
    "\n",
    "        # Get accumulated residuals\n",
    "        accumulated_residual, labels = current_cache.accumulated_resid(\n",
    "            layer=-1, \n",
    "            incl_mid=True,\n",
    "            pos_slice=-1,\n",
    "            return_labels=True\n",
    "        )\n",
    "\n",
    "        numeric_logits_per_layer = get_numeric_logits_per_layer(\n",
    "            accumulated_residual,\n",
    "            current_cache,\n",
    "            numeric_residual_directions\n",
    "        )\n",
    "\n",
    "        numeric_probs_per_layer = t.softmax(numeric_logits_per_layer, dim=-1)\n",
    "\n",
    "        # Build numeric values tensor\n",
    "        numeric_values_list = []\n",
    "        for digit in numerical_tokens.keys():\n",
    "            try:\n",
    "                numeric_values_list.append(float(digit))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        numeric_values_tensor = t.tensor(numeric_values_list, device=numeric_probs_per_layer.device)\n",
    "        numeric_values_reshaped = numeric_values_tensor.view(1, 1, -1)\n",
    "\n",
    "        # Calculate expected values\n",
    "        expected_values_per_layer = (numeric_probs_per_layer * numeric_values_reshaped).sum(dim=-1)\n",
    "        all_expected_values.append(expected_values_per_layer)\n",
    "\n",
    "        # Calculate MSE\n",
    "        gold_values = predictor_tensor\n",
    "        gold_values_expanded = gold_values.unsqueeze(0)\n",
    "        layerwise_mse = (expected_values_per_layer - gold_values_expanded) ** 2\n",
    "        layerwise_mse_mean = layerwise_mse.mean(dim=-1)\n",
    "        all_layerwise_mse.append(layerwise_mse_mean)\n",
    "\n",
    "        # Clear GPU memory\n",
    "        # current_cache = current_cache.to('cpu')\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.empty_cache()\n",
    "\n",
    "    # Average results across all caches\n",
    "    avg_layerwise_mse = t.stack(all_layerwise_mse).mean(dim=0)\n",
    "    avg_expected_values = t.stack(all_expected_values).mean(dim=0)\n",
    "\n",
    "    # Generate plots with averaged results\n",
    "    line(\n",
    "        avg_layerwise_mse,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Average MSE vs. Gold Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"MSE\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    line(\n",
    "        avg_expected_values.mean(dim=-1),  # Average across batch dimension\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Average Expected Numeric Prediction Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Predicted Value\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de06fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    t.cuda.empty_cache()\n",
    "# Example usage\n",
    "(linreg_tokens, linreg_logits, linreg_caches, linreg_data_store) = run_and_cache_model_linreg_tokens_batched(\n",
    "    model,\n",
    "    seq_len=25,\n",
    "    total_batch=1\n",
    ")\n",
    "\n",
    "\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "model.clear_contexts()\n",
    "\n",
    "# Extract predictions across all datasets and convert to tensors\n",
    "predictions_store = {\n",
    "    predictor: t.tensor([\n",
    "        dataset[\"predictions\"][predictor] \n",
    "        for dataset in linreg_data_store\n",
    "    ], dtype=t.float32)\n",
    "    for predictor in linreg_data_store[0][\"predictions\"].keys()\n",
    "}\n",
    "\n",
    "numeric_ids = []\n",
    "numeric_values = []\n",
    "numeric_labels = []\n",
    "for digit, token_id in numerical_tokens.items():\n",
    "    try:\n",
    "        value = float(digit)\n",
    "        numeric_ids.append(token_id)\n",
    "        numeric_values.append(value)\n",
    "        numeric_labels.append(str(value))\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "# Print predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(\"-\" * 50)\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\n{predictor_name}:\")\n",
    "    for i, pred in enumerate(predictor_tensor):\n",
    "        print(f\"  Sample {i}: {pred:.2f}\")\n",
    "\n",
    "# Process each predictor\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\nPredictor: {predictor_name}\")\n",
    "    print(f\"Shape: {predictor_tensor.shape}\")\n",
    "    print(f\"Values: {predictor_tensor}\")\n",
    "\n",
    "    numerical_token_ids = t.tensor(list(numerical_tokens.values()), device=\"cpu\").expand(predictor_tensor.shape[0], -1)\n",
    "    print(f\"Numerical token IDs shape: {numerical_token_ids.shape}\")\n",
    "\n",
    "\n",
    "    def logits_to_numeric_mse(\n",
    "        logits_list: List[Float[Tensor, \"batch seq d_vocab\"]],\n",
    "        predicted_values: Float[Tensor, \"batch\"],\n",
    "        numerical_tokens: dict,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        '''\n",
    "        Handles logits with potentially different sequence lengths\n",
    "        '''\n",
    "        # Get final token logits from each batch\n",
    "        final_logits_list = [logits[:, -1, :] for logits in logits_list]\n",
    "        # Combine final logits\n",
    "        combined_final_logits = t.cat(final_logits_list, dim=0)\n",
    "        \n",
    "        numeric_ids = []\n",
    "        numeric_values = []\n",
    "        numeric_labels = []\n",
    "        for digit, token_id in numerical_tokens.items():\n",
    "            try:\n",
    "                value = float(digit)\n",
    "                numeric_ids.append(token_id)\n",
    "                numeric_values.append(value)\n",
    "                numeric_labels.append(str(value))\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        numeric_ids = t.tensor(numeric_ids, device=combined_final_logits.device)\n",
    "        numeric_values = t.tensor(numeric_values, device=combined_final_logits.device)\n",
    "        \n",
    "        numeric_logits = combined_final_logits[:, numeric_ids]\n",
    "        numeric_probs = t.softmax(numeric_logits, dim=-1)\n",
    "        \n",
    "        expected_values = (numeric_probs * numeric_values.unsqueeze(0)).sum(dim=-1)\n",
    "        \n",
    "        mse = (expected_values - predicted_values) ** 2\n",
    "        \n",
    "        if per_prompt:\n",
    "            return mse\n",
    "        else:\n",
    "            return mse.mean()\n",
    "\n",
    "\n",
    "    original_per_prompt_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens, per_prompt=True)\n",
    "    original_average_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens)\n",
    "\n",
    "    # Initialize lists to store results from all caches\n",
    "    all_layerwise_mse = []\n",
    "    all_expected_values = []\n",
    "    all_probs_per_layer = []  # New list to store probabilities\n",
    "\n",
    "    # Process each cache\n",
    "    for cache_idx, current_cache in enumerate(linreg_caches):\n",
    "        logger.info(f\"Processing cache {cache_idx}\")\n",
    "        \n",
    "        # Move cache to GPU\n",
    "        # current_cache = current_cache.to('cuda')\n",
    "\n",
    "        # Compute residual directions for numeric MSE\n",
    "        numeric_residual_directions = model.tokens_to_residual_directions(numerical_token_ids).to(\"cpu\")\n",
    "\n",
    "        # Get accumulated residuals\n",
    "        accumulated_residual, labels = current_cache.accumulated_resid(\n",
    "            layer=-1, \n",
    "            incl_mid=True,\n",
    "            pos_slice=-1,\n",
    "            return_labels=True\n",
    "        )\n",
    "\n",
    "        numeric_logits_per_layer = get_numeric_logits_per_layer(\n",
    "            accumulated_residual,\n",
    "            current_cache,\n",
    "            numeric_residual_directions\n",
    "        )\n",
    "\n",
    "        numeric_probs_per_layer = t.softmax(numeric_logits_per_layer, dim=-1)\n",
    "        all_probs_per_layer.append(numeric_probs_per_layer)\n",
    "\n",
    "        def get_numeric_logits_per_layer(\n",
    "            residual_stack: Float[Tensor, \"layers batch d_model\"],\n",
    "            cache: ActivationCache,\n",
    "            numeric_residual_directions: Float[Tensor, \"batch num_tokens d_model\"]\n",
    "        ) -> Float[Tensor, \"layers batch num_tokens\"]:\n",
    "            scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "            scaled_residual_stream = scaled_residual_stream.to(\"cpu\")\n",
    "            numeric_residual_directions = numeric_residual_directions.to(dtype=scaled_residual_stream.dtype)\n",
    "\n",
    "            numeric_logits = einops.einsum(\n",
    "                scaled_residual_stream,\n",
    "                numeric_residual_directions,\n",
    "                \"layer b d, b t d -> layer b t\"\n",
    "            )\n",
    "            return numeric_logits\n",
    "\n",
    "        # Get accumulated residuals\n",
    "        accumulated_residual, labels = current_cache.accumulated_resid(\n",
    "            layer=-1, \n",
    "            incl_mid=True,\n",
    "            pos_slice=-1,\n",
    "            return_labels=True\n",
    "        )\n",
    "\n",
    "        numeric_logits_per_layer = get_numeric_logits_per_layer(\n",
    "            accumulated_residual,\n",
    "            current_cache,\n",
    "            numeric_residual_directions\n",
    "        )\n",
    "\n",
    "        numeric_probs_per_layer = t.softmax(numeric_logits_per_layer, dim=-1)\n",
    "\n",
    "        # Build numeric values tensor\n",
    "        numeric_values_list = []\n",
    "        for digit in numerical_tokens.keys():\n",
    "            try:\n",
    "                numeric_values_list.append(float(digit))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        numeric_values_tensor = t.tensor(numeric_values_list, device=numeric_probs_per_layer.device)\n",
    "        numeric_values_reshaped = numeric_values_tensor.view(1, 1, -1)\n",
    "\n",
    "        # Calculate expected values\n",
    "        expected_values_per_layer = (numeric_probs_per_layer * numeric_values_reshaped).sum(dim=-1)\n",
    "        all_expected_values.append(expected_values_per_layer)\n",
    "\n",
    "        # Calculate MSE\n",
    "        gold_values = predictor_tensor\n",
    "        gold_values_expanded = gold_values.unsqueeze(0)\n",
    "        layerwise_mse = (expected_values_per_layer - gold_values_expanded) ** 2\n",
    "        layerwise_mse_mean = layerwise_mse.mean(dim=-1)\n",
    "        all_layerwise_mse.append(layerwise_mse_mean)\n",
    "\n",
    "        # Clear GPU memory\n",
    "        # current_cache = current_cache.to('cpu')\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.empty_cache()\n",
    "\n",
    "    # Average results across all caches\n",
    "    avg_layerwise_mse = t.stack(all_layerwise_mse).mean(dim=0)\n",
    "    avg_expected_values = t.stack(all_expected_values).mean(dim=0)\n",
    "    avg_probs_per_layer = t.stack(all_probs_per_layer).mean(dim=0)\n",
    "\n",
    "# Create probability distribution heatmap\n",
    "    avg_probs = avg_probs_per_layer.mean(dim=1)  # Average across batches\n",
    "    df = pd.DataFrame(\n",
    "        avg_probs.cpu().numpy(),\n",
    "        columns=numeric_labels,\n",
    "        index=labels\n",
    "    )\n",
    "\n",
    "    # Heatmap visualization\n",
    "    fig = px.imshow(\n",
    "        df,\n",
    "        title=f\"Token Probability Distribution Across Layers for {predictor_name}\",\n",
    "        labels=dict(x=\"Numeric Token\", y=\"Layer\", color=\"Probability\"),\n",
    "        aspect=\"auto\",\n",
    "        color_continuous_scale=\"viridis\"\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        xaxis_tickangle=-45,\n",
    "    )\n",
    "\n",
    "    # Add target value marker\n",
    "    target_value = predictor_tensor.mean().item()\n",
    "    closest_token_idx = min(range(len(numeric_values_list)), \n",
    "                          key=lambda i: abs(numeric_values_list[i] - target_value))\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        x=closest_token_idx,\n",
    "        y=-0.5,\n",
    "        text=f\"Target  {target_value:.2f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        yanchor=\"bottom\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Calculate and plot entropy\n",
    "    entropy = -(avg_probs_per_layer * t.log(avg_probs_per_layer + 1e-10)).sum(dim=-1)\n",
    "    avg_entropy = entropy.mean(dim=-1)\n",
    "\n",
    "    line(\n",
    "        avg_entropy,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Distribution Entropy Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Entropy\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    # Print top-k predictions per layer\n",
    "    k = 5\n",
    "    for layer_idx, layer_name in enumerate(labels):\n",
    "        probs = avg_probs[layer_idx]\n",
    "        values = t.tensor(numeric_values_list)\n",
    "        \n",
    "        top_probs, top_indices = t.topk(probs, k)\n",
    "        \n",
    "        print(f\"\\nLayer {layer_name} top {k} predictions:\")\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            print(f\"Value: {numeric_values_list[idx]:.2f}, Probability: {prob:.3f}\")\n",
    "\n",
    "    # Original MSE and expected value plots\n",
    "    line(\n",
    "        avg_layerwise_mse,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Average MSE vs. Gold Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"MSE\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    line(\n",
    "        avg_expected_values.mean(dim=-1),\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Average Expected Numeric Prediction Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Predicted Value\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ab5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the probability distribution visualization outside the predictor loop\n",
    "# First collect all target values\n",
    "target_values = {}\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    target_values[predictor_name] = predictor_tensor.mean().item()\n",
    "\n",
    "# Sort numeric values and labels\n",
    "sorted_indices = np.argsort(numeric_values_list)\n",
    "sorted_numeric_values = [numeric_values_list[i] for i in sorted_indices]\n",
    "sorted_numeric_labels = [numeric_labels[i] for i in sorted_indices]\n",
    "\n",
    "# After the predictor loop ends, create the consolidated visualization\n",
    "avg_probs = avg_probs_per_layer.mean(dim=1)  # Average across batches\n",
    "# Reorder the columns according to sorted indices\n",
    "avg_probs = avg_probs[:, sorted_indices]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    avg_probs.cpu().numpy(),\n",
    "    columns=sorted_numeric_labels,\n",
    "    index=labels\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "fig = px.imshow(\n",
    "    df,\n",
    "    title=\"Token Probability Distribution Across Layers\",\n",
    "    labels=dict(x=\"Numeric Token\", y=\"Layer\", color=\"Probability\"),\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"viridis\"\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    xaxis_tickangle=-45,\n",
    ")\n",
    "\n",
    "# Add target value markers for all predictors\n",
    "colors = px.colors.qualitative.Set1  # Different colors for different predictors\n",
    "for i, (predictor_name, target_value) in enumerate(target_values.items()):\n",
    "    closest_token_idx = min(range(len(sorted_numeric_values)), \n",
    "                          key=lambda i: abs(sorted_numeric_values[i] - target_value))\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        x=closest_token_idx,\n",
    "        y=-0.5 - (i * 0.5),  # Stack annotations vertically\n",
    "        text=f\"{predictor_name}  {target_value:.2f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        yanchor=\"bottom\",\n",
    "        arrowcolor=colors[i % len(colors)],\n",
    "        font=dict(color=colors[i % len(colors)])\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9763434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the probability distribution visualization outside the predictor loop\n",
    "# First collect all target values\n",
    "target_values = {}\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    target_values[predictor_name] = predictor_tensor.mean().item()\n",
    "\n",
    "# After the predictor loop ends, create the consolidated visualization\n",
    "avg_probs = avg_probs_per_layer.mean(dim=1)  # Average across batches\n",
    "# Select only layers 0 to 15\n",
    "avg_probs = avg_probs[:16]  # Since indexing is 0-based\n",
    "labels = labels[:16]  # Adjust labels accordingly\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    avg_probs.cpu().numpy(),\n",
    "    columns=numeric_labels,\n",
    "    index=labels\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "fig = px.imshow(\n",
    "    df,\n",
    "    title=\"Token Probability Distribution Across Layers\",\n",
    "    labels=dict(x=\"Numeric Token\", y=\"Layer\", color=\"Probability\"),\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"viridis\"\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    xaxis_tickangle=-45,\n",
    ")\n",
    "\n",
    "# Add target value markers for all predictors\n",
    "colors = px.colors.qualitative.Set1  # Different colors for different predictors\n",
    "for i, (predictor_name, target_value) in enumerate(target_values.items()):\n",
    "    closest_token_idx = min(range(len(numeric_values_list)), \n",
    "                          key=lambda i: abs(numeric_values_list[i] - target_value))\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        x=closest_token_idx,\n",
    "        y=-0.5 - (i * 0.5),  # Stack annotations vertically\n",
    "        text=f\"{predictor_name}  {target_value:.2f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        yanchor=\"bottom\",\n",
    "        arrowcolor=colors[i % len(colors)],\n",
    "        font=dict(color=colors[i % len(colors)])\n",
    "    )\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Move the probability distribution visualization outside the predictor loop\n",
    "# First collect all target values\n",
    "target_values = {}\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    target_values[predictor_name] = predictor_tensor.mean().item()\n",
    "\n",
    "# After the predictor loop ends, create the consolidated visualization\n",
    "avg_probs = avg_probs_per_layer.mean(dim=1)  # Average across batches\n",
    "# Select only layers 0 to 15\n",
    "avg_probs = avg_probs[16:]  # Since indexing is 0-based\n",
    "labels = labels[16:]  # Adjust labels accordingly\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    avg_probs.cpu().numpy(),\n",
    "    columns=numeric_labels,\n",
    "    index=labels\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "fig = px.imshow(\n",
    "    df,\n",
    "    title=\"Token Probability Distribution Across Layers\",\n",
    "    labels=dict(x=\"Numeric Token\", y=\"Layer\", color=\"Probability\"),\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"viridis\"\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    xaxis_tickangle=-45,\n",
    ")\n",
    "\n",
    "# Add target value markers for all predictors\n",
    "colors = px.colors.qualitative.Set1  # Different colors for different predictors\n",
    "for i, (predictor_name, target_value) in enumerate(target_values.items()):\n",
    "    closest_token_idx = min(range(len(numeric_values_list)), \n",
    "                          key=lambda i: abs(numeric_values_list[i] - target_value))\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        x=closest_token_idx,\n",
    "        y=-0.5 - (i * 0.5),  # Stack annotations vertically\n",
    "        text=f\"{predictor_name}  {target_value:.2f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        yanchor=\"bottom\",\n",
    "        arrowcolor=colors[i % len(colors)],\n",
    "        font=dict(color=colors[i % len(colors)])\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the probability distribution visualization outside the predictor loop\n",
    "# First collect all target values\n",
    "target_values = {}\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    target_values[predictor_name] = predictor_tensor.mean().item()\n",
    "\n",
    "# After the predictor loop ends, create the consolidated visualization\n",
    "avg_probs = avg_probs_per_layer.mean(dim=1)  # Average across batches\n",
    "# Select only layers 0 to 15\n",
    "avg_probs = avg_probs[:]  # Since indexing is 0-based\n",
    "labels = labels[:]  # Adjust labels accordingly\n",
    "\n",
    "# Sort numeric values and create sorted indices\n",
    "numeric_values_array = np.array([float(label) for label in numeric_labels])\n",
    "sorted_indices = np.argsort(numeric_values_array)\n",
    "numeric_labels_sorted = [numeric_labels[i] for i in sorted_indices]\n",
    "\n",
    "# Reorder the probabilities according to sorted tokens\n",
    "avg_probs_sorted = avg_probs[:, sorted_indices]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    avg_probs_sorted.cpu().numpy(),\n",
    "    columns=numeric_labels_sorted,\n",
    "    index=labels\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "fig = px.imshow(\n",
    "    df,\n",
    "    title=\"Token Probability Distribution Across Layers\",\n",
    "    labels=dict(x=\"Numeric Token\", y=\"Layer\", color=\"Probability\"),\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"viridis\"\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    xaxis_tickangle=-45,\n",
    ")\n",
    "\n",
    "# Add target value markers for all predictors\n",
    "colors = px.colors.qualitative.Set1  # Different colors for different predictors\n",
    "for i, (predictor_name, target_value) in enumerate(target_values.items()):\n",
    "    closest_token_idx = min(range(len(numeric_values_array)), \n",
    "                          key=lambda i: abs(float(numeric_labels_sorted[i]) - target_value))\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        x=closest_token_idx,\n",
    "        y=-0.5 - (i * 0.5),  # Stack annotations vertically\n",
    "        text=f\"{predictor_name}  {target_value:.2f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        yanchor=\"bottom\",\n",
    "        arrowcolor=colors[i % len(colors)],\n",
    "        font=dict(color=colors[i % len(colors)])\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57279b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m einsum\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = HookedTransformer.from_pretrained(\"gpt2-small\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create input with numerical tokens\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "from einops import einsum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load model\n",
    "# model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "\n",
    "# Create input with numerical tokens\n",
    "prompt = \"The number is 42. The next number is\"\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "\n",
    "# Get accumulated residual streams for each layer\n",
    "accum_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "\n",
    "# Get the last token position\n",
    "last_pos = -1\n",
    "last_token_accum = accum_resid[:, 0, last_pos, :]  # [layer, d_model]\n",
    "\n",
    "# Project into logit space\n",
    "W_U = model.W_U\n",
    "logit_by_layer = einsum(\n",
    "    last_token_accum,\n",
    "    W_U,\n",
    "    \"layer d_model, d_model d_vocab -> layer d_vocab\"\n",
    ")\n",
    "\n",
    "# Get probabilities using softmax\n",
    "probs_by_layer = torch.softmax(logit_by_layer, dim=-1)\n",
    "\n",
    "# Get indices of numerical tokens (0-9)\n",
    "number_tokens = [model.to_single_token(str(i)) for i in range(10)]\n",
    "\n",
    "# Extract probabilities for numerical tokens\n",
    "number_probs = probs_by_layer[:, number_tokens]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    number_probs.cpu().numpy(),\n",
    "    xticklabels=range(10),\n",
    "    yticklabels=labels,\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Probability Distribution over Numerical Tokens by Layer')\n",
    "plt.show()\n",
    "\n",
    "# Print top predictions per layer\n",
    "for layer, label in enumerate(labels):\n",
    "    top_numbers = torch.topk(number_probs[layer], k=3)\n",
    "    print(f\"\\nLayer {label}:\")\n",
    "    for prob, idx in zip(top_numbers.values, top_numbers.indices):\n",
    "        print(f\"Number {idx.item()}: {prob.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
