{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "from optim_hunter.plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference, line\n",
    "from optim_hunter.utils import prepare_prompt, slice_dataset\n",
    "from optim_hunter.sklearn_regressors import linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random\n",
    "from optim_hunter.datasets import get_dataset_friedman_2\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "#device = t.device(\"cuda:0,1\" if t.cuda.is_available() else \"cpu\")\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# device = t.device(\"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directly from model path https://github.com/TransformerLensOrg/TransformerLens/issues/691\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "MODEL_PATH = \"/home/freiza/optim_hunter/.models/Llama-3.1-8B-Instruct/\"\n",
    "\n",
    "if MODEL_PATH:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True,\n",
    "                                                     #quantization_config=BitsAndBytesConfig(load_in_4bit=True), \n",
    "                                                     #torch_dtype = t.float32, \n",
    "                                                     #device_map = \"cuda:0\"\n",
    "                                                     )\n",
    "\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        MODEL_TYPE,\n",
    "        hf_model=hf_model,\n",
    "        device=\"cuda\",\n",
    "        n_devices=2,\n",
    "        fold_ln=True,\n",
    "        # fold_value_biases=False,\n",
    "        center_writing_weights=True,\n",
    "        # refactor_factored_attn_matrices=True,\n",
    "        center_unembed=True,\n",
    "        # dtype=t.bfloat16,\n",
    "        dtype=t.float16,\n",
    "        default_padding_side='left',\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    #model = model.to(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "    #model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "print(\"Found numerical tokens:\", numerical_tokens)\n",
    "print(\"Number of numerical tokens:\", len(numerical_tokens))\n",
    "\n",
    "\n",
    "# Check if digit 0 is in numerical tokens\n",
    "has_zero = '0' in numerical_tokens\n",
    "print(\"Zero token present:\", has_zero)\n",
    "if has_zero:\n",
    "    print(\"Token ID for zero:\", numerical_tokens['29'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65997326",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "x_train, y_train, x_test, y_test = get_dataset_friedman_2(random_state=11)\n",
    "# x_train, y_train, x_test, y_test =  slice_dataset(x_train, y_train, x_test, y_test, seq_len)\n",
    "linear_regression_prediction = linear_regression(x_train, x_test, y_train, y_test)['y_predict']\n",
    "knn_regression_prediction = knn_regression(x_train, x_test, y_train, y_test)['y_predict']\n",
    "random_forest_prediction = random_forest(x_train, x_test, y_train, y_test)['y_predict']\n",
    "baseline_avg_prediction = baseline_average(x_train, x_test, y_train, y_test)['y_predict']\n",
    "baseline_last_prediction = baseline_last(x_train, x_test, y_train, y_test)['y_predict']\n",
    "baseline_random_prediction = baseline_random(x_train, x_test, y_train, y_test)['y_predict']\n",
    "gold = y_test.values[0]\n",
    "\n",
    "print(\"Linear Regression Prediction:\", linear_regression_prediction[0])\n",
    "print(\"Random Forest Prediction:\", random_forest_prediction[0])\n",
    "print(\"KNN Regression Prediction:\", knn_regression_prediction[0])\n",
    "print(\"Baseline Average Prediction:\", baseline_avg_prediction[0])\n",
    "print(\"Baseline Last Prediction:\", baseline_last_prediction[0]) \n",
    "print(\"Baseline Random Prediction:\", baseline_random_prediction[0])\n",
    "print(\"Gold:\", gold)\n",
    "\n",
    "prompt = prepare_prompt(x_train, y_train, x_test)\n",
    "prompt = prompt + \"\"\n",
    "# example_answer = f\"{baseline_last_prediction[0]}\"\n",
    "# example_answer = f\"{random_forest_prediction[0]}\"\n",
    "example_answer = f\"{gold}\"\n",
    "utils.test_prompt(prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f361e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_data(model, dataset_func, regressors, random_state=1, seq_len=None):\n",
    "    \"\"\"\n",
    "    Creates a structured comparison dataset for analyzing different regression models against gold values.\n",
    "    \n",
    "    Args:\n",
    "        model (HookedTransformer): The transformer model used for tokenization\n",
    "        dataset_func (callable): Function that returns (x_train, y_train, x_test, y_test)\n",
    "        regressors (list): List of regression functions to compare\n",
    "        random_state (int, optional): Random seed for dataset generation. Defaults to 11\n",
    "    \n",
    "    Returns:\n",
    "        dict: A structured dictionary containing:\n",
    "        {\n",
    "            'dataset_name': str,  # Name of the dataset function\n",
    "            'prompt': str,        # Generated prompt text for the model\n",
    "            'predictions': {      # Dictionary of predictions from each model\n",
    "                'gold': float,    # True value\n",
    "                'model_name1': float,  # Prediction from first model\n",
    "                'model_name2': float,  # Prediction from second model\n",
    "                ...\n",
    "            },\n",
    "            'comparison_names': [  # List of comparison descriptors\n",
    "                'model1 vs model2',\n",
    "                'model1 vs model3',\n",
    "                ...\n",
    "            ],\n",
    "            'token_pairs': tensor  # Shape: [num_comparisons, 1, 2]\n",
    "                                  # Each pair contains the first tokens of two predictions\n",
    "                                  # being compared\n",
    "        }\n",
    "    \n",
    "    Note:\n",
    "        - The function generates unique combinations (not permutations) of comparisons\n",
    "        - Only the first token of each prediction is stored in token_pairs\n",
    "        - All possible combinations between gold and regressors are included\n",
    "        - Token pairs maintain the order specified in comparison_names\n",
    "    \"\"\"\n",
    "    # Get dataset\n",
    "    x_train, y_train, x_test, y_test = dataset_func(random_state=random_state)\n",
    "    if seq_len:\n",
    "        x_train, y_train, x_test, y_test = slice_dataset(x_train, y_train, x_test, y_test, seq_len)\n",
    "    \n",
    "    # Get prompt\n",
    "    prompt = prepare_prompt(x_train, y_train, x_test)\n",
    "    \n",
    "    # Get gold value\n",
    "    gold = y_test.values[0]\n",
    "    \n",
    "    # Get predictions from each regressor\n",
    "    predictions = {}\n",
    "    predictions['gold'] = gold\n",
    "    if regressors:\n",
    "        for regressor in regressors:\n",
    "            result = regressor(x_train, x_test, y_train, y_test)\n",
    "            predictions[result['model_name']] = result['y_predict'][0]\n",
    "    \n",
    "    # Create comparison names and token pairs\n",
    "    comparison_names = []\n",
    "    token_pairs = []\n",
    "    \n",
    "    # Create list of all predictors (including gold)\n",
    "    if regressors:\n",
    "        all_predictors = ['gold'] + [reg(x_train, x_test, y_train, y_test)['model_name'] for reg in regressors]\n",
    "    else:\n",
    "        all_predictors = ['gold']\n",
    "\n",
    "    # Generate unique combinations (not permutations)\n",
    "    for i, pred1 in enumerate(all_predictors):\n",
    "        for j, pred2 in enumerate(all_predictors[i+1:], i+1):  # Start from i+1 to avoid duplicates\n",
    "            comparison_name = f\"{pred1} vs {pred2}\"\n",
    "            comparison_names.append(comparison_name)\n",
    "            \n",
    "            # Tokenize each prediction separately and get their first tokens\n",
    "            tokens1 = model.to_tokens(str(predictions[pred1]), prepend_bos=False)[0, 0]  # First token of first prediction\n",
    "            tokens2 = model.to_tokens(str(predictions[pred2]), prepend_bos=False)[0, 0]  # First token of second prediction\n",
    "            \n",
    "            # Combine the first tokens into a pair\n",
    "            first_tokens = t.tensor([tokens1, tokens2], device=tokens1.device).unsqueeze(0)  # Shape: [1, 2]\n",
    "            token_pairs.append(first_tokens)\n",
    "\n",
    "    # Verification Step: Ensure that each comparison_name matches the corresponding token_pair\n",
    "    for idx, (comp_name, token_pair) in enumerate(zip(comparison_names, token_pairs)):\n",
    "        pred1_name, pred2_name = comp_name.split(' vs ')\n",
    "        pred1_value = predictions[pred1_name]\n",
    "        pred2_value = predictions[pred2_name]\n",
    "        \n",
    "        # Tokenize the actual prediction values\n",
    "        actual_tokens1 = model.to_tokens(str(pred1_value), prepend_bos=False)[0, 0].item()\n",
    "        actual_tokens2 = model.to_tokens(str(pred2_value), prepend_bos=False)[0, 0].item()\n",
    "        \n",
    "        # Extract tokens from token_pair\n",
    "        token1, token2 = token_pair.squeeze(0).tolist()\n",
    "        \n",
    "        # Assert that tokens match\n",
    "        assert token1 == actual_tokens1, f\"Mismatch in token1 for comparison '{comp_name}' at index {idx}\"\n",
    "        assert token2 == actual_tokens2, f\"Mismatch in token2 for comparison '{comp_name}' at index {idx}\"\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'dataset_name': dataset_func.__name__,\n",
    "        'prompt': prompt,\n",
    "        'predictions': predictions,\n",
    "        'comparison_names': comparison_names,\n",
    "        'token_pairs': t.stack(token_pairs),  # Shape: [num_comparisons, 1, 2]\n",
    "    }\n",
    "\n",
    "# # Create the data store\n",
    "# datasets = [ get_dataset_friedman_2 ]\n",
    "# regressors = [ linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random ]\n",
    "# data_store = {}\n",
    "# for dataset_func in datasets:\n",
    "#     data_store[dataset_func.__name__] = create_comparison_data(model, dataset_func, regressors)\n",
    "# # Print out the token pairs and comparison names\n",
    "# for dataset_name, dataset_info in data_store.items():\n",
    "#     print(f\"\\nDataset: {dataset_name}\")\n",
    "#     print(\"Token pairs:\")\n",
    "#     print(dataset_info['token_pairs'])\n",
    "#     print(\"\\nComparison names:\")\n",
    "#     print(dataset_info['comparison_names'])\n",
    "\n",
    "# # Get the first token pair from the first dataset\n",
    "# first_dataset_name = next(iter(data_store))\n",
    "# first_token_pair = data_store[first_dataset_name]['token_pairs'][0]  # Shape: [1, 2]\n",
    "# print(\"\\nFirst token pair:\")\n",
    "# print(first_token_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91e6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [ linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random ]\n",
    "\n",
    "def generate_linreg_tokens(\n",
    "    model: HookedTransformer,\n",
    "    dataset,\n",
    "    seq_len = 5,\n",
    "    batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    '''\n",
    "    Generates a sequence of linear regression ICL tokens\n",
    "\n",
    "    Outputs are:\n",
    "        linreg_tokens: [batch, 1+linreg]\n",
    "    '''\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long().to(device)\n",
    "    zero_token = model.to_tokens('0', truncate=True)[0][-1]\n",
    "    \n",
    "    # Create list to store tokens for each batch\n",
    "    batch_tokens = []\n",
    "    data_store = []\n",
    "\n",
    "    dataset_func = get_dataset_friedman_2\n",
    "    \n",
    "    # Generate tokens for each batch with different random seeds\n",
    "    for i in range(batch):\n",
    "        data = create_comparison_data(model, dataset_func, regressors, random_state=i, seq_len=seq_len)\n",
    "        tokens = model.to_tokens(data['prompt'], truncate=True)\n",
    "        batch_tokens.append(tokens[0])\n",
    "        data_store.append(data)\n",
    "    \n",
    "    # Find the longest sequence length\n",
    "    max_len = max(len(tokens) for tokens in batch_tokens)\n",
    "    \n",
    "    # Pad shorter sequences with token 0 at position -4\n",
    "    for i in range(len(batch_tokens)):\n",
    "        while len(batch_tokens[i]) < max_len:\n",
    "            # Insert 0 at position -4 from the end\n",
    "            print(f\"Found mismatch in token length for batch {i}!\\nLargest length: {max_len}\\nBatch {i} length: {len(batch_tokens[i])}\\nApplying padding...\")\n",
    "            print(f\"\\nBefore Zero Token Padding:\\n#####\\n{model.to_string(batch_tokens[i][-50:])}\\n#####\")\n",
    "            batch_tokens[i] = t.cat([\n",
    "                batch_tokens[i][:len(batch_tokens[i])-3],  \n",
    "                zero_token.unsqueeze(0), # Add unsqueeze to make zero_token 1-dimensional\n",
    "                batch_tokens[i][len(batch_tokens[i])-3:]\n",
    "            ])\n",
    "            print(f\"\\nAfter Zero Token Padding:\\n#####\\n{model.to_string(batch_tokens[i][-50:])}\\n#####\")\n",
    "\n",
    "    \n",
    "    # Stack all batches together \n",
    "    linreg_tokens = t.stack(batch_tokens).to(device)\n",
    "    \n",
    "    # Add prefix to each batch\n",
    "    linreg_tokens = t.cat([prefix, linreg_tokens], dim=-1).to(device)\n",
    "    return linreg_tokens, data_store\n",
    "\n",
    "def run_and_cache_model_linreg_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "    '''\n",
    "    Generates a sequence of linear regression ICL tokens, and runs the model on it, returning (tokens, logits, cache)\n",
    "\n",
    "    Should use the `generate_linreg_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        linreg_tokens: [batch, 1+linreg]\n",
    "        linreg_logits: [batch, 1+linreg, d_vocab]\n",
    "        linreg_cache: The cache of the model run on linreg_tokens\n",
    "    '''\n",
    "    linreg_tokens, linreg_data_store = generate_linreg_tokens(model, get_dataset_friedman_2, seq_len, batch)\n",
    "    linreg_logits, linreg_cache = model.run_with_cache(linreg_tokens)\n",
    "    return linreg_tokens, linreg_logits, linreg_cache, linreg_data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_compare_predictions(model, dataset_func, regressors, num_samples=5, seq_len=None):\n",
    "    \"\"\"\n",
    "    Generate model predictions and compare against regression baselines using MSE across multiple prompts\n",
    "    \n",
    "    Args:\n",
    "        model (HookedTransformer): The transformer model\n",
    "        dataset_func (callable): Function that returns dataset splits\n",
    "        regressors (list): List of regression functions to compare\n",
    "        num_samples (int): Number of different prompts to generate and test\n",
    "        seq_len (int, optional): Length to slice dataset\n",
    "        \n",
    "    Returns:\n",
    "        dict: MSE scores and predictions for each sample\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # Generate multiple samples\n",
    "    for i in range(num_samples):\n",
    "        # Get data and predictions using create_comparison_data\n",
    "        data = create_comparison_data(model, dataset_func, regressors, random_state=i, seq_len=seq_len)\n",
    "        \n",
    "        # Get the prompt\n",
    "        prompt = data['prompt']\n",
    "        \n",
    "        # Generate model prediction\n",
    "        pred_text = model.generate(prompt, max_new_tokens=4, temperature=0)\n",
    "        # No need to convert to string since generate() returns string directly\n",
    "        \n",
    "        # Extract the numeric prediction from the generated text\n",
    "        try:\n",
    "            # Clean the prediction text - remove the prompt and keep only the generated part\n",
    "            # This assumes the model's output follows the prompt\n",
    "            generated_part = pred_text.replace(prompt, '').strip()\n",
    "            model_pred = float(generated_part)\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not parse model prediction for sample {i}: {pred_text}\")\n",
    "            model_pred = None\n",
    "            \n",
    "        # Get gold value and regression predictions\n",
    "        sample_results = {\n",
    "            'sample_id': i,\n",
    "            'predictions': {\n",
    "                'llama': model_pred,  # Changed 'model' to 'llama' for clarity\n",
    "                'gold': data['predictions']['gold'],\n",
    "            },\n",
    "            'mse_scores': {}\n",
    "        }\n",
    "        \n",
    "        # Add predictions from all regressors\n",
    "        for reg_name, pred_value in data['predictions'].items():\n",
    "            if reg_name != 'gold':\n",
    "                sample_results['predictions'][reg_name] = pred_value\n",
    "        \n",
    "        # Calculate MSE scores for all predictions including the model's\n",
    "        gold = sample_results['predictions']['gold']\n",
    "        for method, pred in sample_results['predictions'].items():\n",
    "            if method != 'gold' and pred is not None:\n",
    "                sample_results['mse_scores'][method] = (pred - gold) ** 2\n",
    "                \n",
    "        all_results.append(sample_results)\n",
    "    \n",
    "    # Calculate average MSE across all samples\n",
    "    avg_mse = {method: [] for method in all_results[0]['mse_scores'].keys()}\n",
    "    for result in all_results:\n",
    "        for method, mse in result['mse_scores'].items():\n",
    "            avg_mse[method].append(mse)\n",
    "    \n",
    "    avg_mse = {method: sum(scores)/len(scores) for method, scores in avg_mse.items()}\n",
    "    \n",
    "    return {\n",
    "        'individual_results': all_results,\n",
    "        'average_mse': avg_mse\n",
    "    }\n",
    "\n",
    "def plot_comparison_results(results):\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                       subplot_titles=('Individual Sample Predictions', 'Average MSE Across Samples'),\n",
    "                       vertical_spacing=0.3)\n",
    "    \n",
    "    # Colors for different methods\n",
    "    methods = list(results['individual_results'][0]['predictions'].keys())\n",
    "    methods.remove('gold')\n",
    "    colors = px.colors.qualitative.Set3[:len(methods)]\n",
    "    color_map = dict(zip(methods, colors))\n",
    "    \n",
    "    # Plot individual predictions\n",
    "    for method in methods:\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        for sample in results['individual_results']:\n",
    "            x_vals.append(f\"Sample {sample['sample_id']}\")\n",
    "            y_vals.append(sample['predictions'][method])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=method,\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines+markers',\n",
    "                line=dict(color=color_map[method])\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add gold values\n",
    "        if method == methods[0]:  # Only add gold once\n",
    "            gold_vals = [sample['predictions']['gold'] for sample in results['individual_results']]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    name='Gold',\n",
    "                    x=x_vals,\n",
    "                    y=gold_vals,\n",
    "                    mode='lines+markers',\n",
    "                    line=dict(color='black', dash='dash')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # Plot average MSE\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name='Average MSE',\n",
    "            x=list(results['average_mse'].keys()),\n",
    "            y=list(results['average_mse'].values()),\n",
    "            marker_color=[color_map[method] for method in results['average_mse'].keys()]\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Model vs Regression Methods Comparison Across Multiple Samples',\n",
    "        showlegend=True,\n",
    "        height=1000,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update y-axes labels\n",
    "    fig.update_yaxes(title_text=\"Prediction Value\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"MSE\", row=2, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Run comparison\n",
    "regressors = [linear_regression, knn_regression, random_forest, \n",
    "              baseline_average, baseline_last, baseline_random]\n",
    "\n",
    "results = generate_and_compare_predictions(\n",
    "    model=model,\n",
    "    dataset_func=get_dataset_friedman_2,\n",
    "    regressors=regressors,\n",
    "    num_samples=1,\n",
    "    seq_len=None\n",
    ")\n",
    "\n",
    "# Create and display visualization\n",
    "fig = plot_comparison_results(results)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_contexts()\n",
    "\n",
    "seq_len = None\n",
    "# TODO we need to be able to run more batches but not over \n",
    "batch = 1\n",
    "(linreg_tokens, linreg_logits, linreg_cache, linreg_data_store) = run_and_cache_model_linreg_tokens(model, seq_len, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94176e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logit_lens_plot(logit_lens_logit_diffs, labels, comparison_name):\n",
    "    \"\"\"\n",
    "    Creates a lightweight, themed logit lens plot suitable for web embedding.\n",
    "    \n",
    "    Args:\n",
    "        logit_lens_logit_diffs: Tensor of logit differences\n",
    "        labels: List of layer labels\n",
    "        comparison_name: Name of the comparison being plotted\n",
    "    \n",
    "    Returns:\n",
    "        str: HTML/JavaScript code for the plot using Plotly\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Convert tensor to list\n",
    "    logit_diffs = logit_lens_logit_diffs.tolist()\n",
    "    \n",
    "    # Create the trace\n",
    "    trace = go.Scatter(\n",
    "        x=list(range(len(logit_diffs))),\n",
    "        y=logit_diffs,\n",
    "        mode='lines+markers',\n",
    "        line=dict(\n",
    "            color='#9ec5fe',  # Matches website theme\n",
    "            width=2\n",
    "        ),\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color='#9ec5fe',\n",
    "            line=dict(\n",
    "                color='#ffffff',\n",
    "                width=1\n",
    "            )\n",
    "        ),\n",
    "        hovertemplate='Layer: %{x}<br>Logit Diff: %{y:.3f}<extra></extra>'\n",
    "    )\n",
    "\n",
    "    # Create the layout\n",
    "    layout = go.Layout(\n",
    "        template='plotly_dark',  # Dark theme to match website\n",
    "        plot_bgcolor='rgba(26,26,26,0)',  # Transparent background\n",
    "        paper_bgcolor='rgba(26,26,26,0)',\n",
    "        margin=dict(l=50, r=20, t=50, b=50),\n",
    "        xaxis=dict(\n",
    "            title='Layer',\n",
    "            gridcolor='#444',\n",
    "            ticktext=labels,\n",
    "            tickvals=list(range(len(labels))),\n",
    "            tickmode='array',\n",
    "            showgrid=True,\n",
    "            zeroline=False\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Logit Difference',\n",
    "            gridcolor='#444',\n",
    "            showgrid=True,\n",
    "            zeroline=False\n",
    "        ),\n",
    "        title=dict(\n",
    "            text=f'Logit Difference Across Layers<br><sub>{comparison_name}</sub>',\n",
    "            font=dict(\n",
    "                size=14,\n",
    "                color='#e0e0e0'\n",
    "            ),\n",
    "            x=0.5,\n",
    "            xanchor='center'\n",
    "        ),\n",
    "        hoverlabel=dict(\n",
    "            bgcolor='#333',\n",
    "            font_size=12,\n",
    "            font_family=\"monospace\"\n",
    "        ),\n",
    "        width=600,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    \n",
    "    # Add light theme configuration\n",
    "    fig.update_layout(\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            showactive=False,\n",
    "            buttons=[\n",
    "                dict(\n",
    "                    label=\"Light Theme\",\n",
    "                    method=\"relayout\",\n",
    "                    args=[{\n",
    "                        \"plot_bgcolor\": \"rgba(253,246,227,0)\",\n",
    "                        \"paper_bgcolor\": \"rgba(253,246,227,0)\",\n",
    "                        \"font.color\": \"#073642\",\n",
    "                        \"xaxis.gridcolor\": \"#93a1a1\",\n",
    "                        \"yaxis.gridcolor\": \"#93a1a1\",\n",
    "                        \"title.font.color\": \"#073642\"\n",
    "                    }]\n",
    "                ),\n",
    "                dict(\n",
    "                    label=\"Dark Theme\",\n",
    "                    method=\"relayout\",\n",
    "                    args=[{\n",
    "                        \"plot_bgcolor\": \"rgba(26,26,26,0)\",\n",
    "                        \"paper_bgcolor\": \"rgba(26,26,26,0)\",\n",
    "                        \"font.color\": \"#e0e0e0\",\n",
    "                        \"xaxis.gridcolor\": \"#444\",\n",
    "                        \"yaxis.gridcolor\": \"#444\",\n",
    "                        \"title.font.color\": \"#e0e0e0\"\n",
    "                    }]\n",
    "                )\n",
    "            ],\n",
    "            x=0.9,\n",
    "            y=1.1,\n",
    "            xanchor=\"right\",\n",
    "            yanchor=\"top\",\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    # Generate minimal HTML\n",
    "    plot_html = fig.to_html(\n",
    "        full_html=False,\n",
    "        include_plotlyjs='cdn',\n",
    "        config={'displayModeBar': False}\n",
    "    )\n",
    "    \n",
    "    return plot_html\n",
    "\n",
    "# # Modify your existing loop to use the new plotting function\n",
    "# for i, token_pair in enumerate(token_pairs):\n",
    "#     logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, linreg_cache)\n",
    "    # plot_html = create_logit_lens_plot(\n",
    "    #     logit_lens_logit_diffs,\n",
    "    #     labels,\n",
    "    #     token_pairs_names[i]\n",
    "    # )\n",
    "    \n",
    "    # # Save just the plot HTML\n",
    "    # output_path = f\"../docs/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\"\n",
    "    # with open(output_path, 'w') as f:\n",
    "    #     f.write(plot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_contexts()\n",
    "linreg_tokens = linreg_tokens.to('cpu')\n",
    "linreg_logits = linreg_logits.to('cpu')\n",
    "linreg_cache = linreg_cache.to('cpu')\n",
    "\n",
    "# Verify all datasets have the same comparison names\n",
    "base_comparison_names = linreg_data_store[0][\"comparison_names\"]\n",
    "all_match = all(dataset[\"comparison_names\"] == base_comparison_names for dataset in linreg_data_store[1:])\n",
    "assert all_match, \"Mismatch in comparison names across datasets.\"\n",
    "\n",
    "# Extract comparison names from the first dataset\n",
    "token_pairs_names = base_comparison_names.copy()\n",
    "\n",
    "# Extract token pairs across all datasets for each comparison\n",
    "token_pairs = [\n",
    "    t.stack([dataset[\"token_pairs\"][i] for dataset in linreg_data_store])[0]\n",
    "    for i in range(len(token_pairs_names))\n",
    "]\n",
    "\n",
    "logger.info(f\"Number of comparisons: {len(token_pairs_names)}\")\n",
    "logger.info(f\"Number of token_pairs: {len(token_pairs)}\")\n",
    "\n",
    "# Iterate over token pairs and generate plots\n",
    "for i, token_pair in enumerate(token_pairs):\n",
    "    logger.info(f\"Processing comparison {i}: {token_pairs_names[i]}\")\n",
    "    token_pair = token_pair.to('cpu')\n",
    "\n",
    "    # Print token pair shape for debugging\n",
    "    print(f\"Token pair shape: {token_pair.shape}\")\n",
    "\n",
    "    def logits_to_ave_logit_diff(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        answer_tokens: Float[Tensor, \"batch 2\"] = token_pair,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        '''\n",
    "        Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "        If per_prompt=True, return the array of differences rather than the average.\n",
    "        '''\n",
    "        # Extract token IDs for correct and incorrect answers\n",
    "        correct = answer_tokens[:, 0]  # Correct token IDs\n",
    "        incorrect = answer_tokens[:, 1]  # Incorrect token IDs\n",
    "\n",
    "        # Extract logits for the final token in the sequence\n",
    "        final_logits = logits[:, -1, :]  # Shape: (batch, d_vocab)\n",
    "\n",
    "        # Get logits for the correct and incorrect answers\n",
    "        correct_logits = final_logits[t.arange(final_logits.size(0)), correct]  # Shape: (batch,)\n",
    "        incorrect_logits = final_logits[t.arange(final_logits.size(0)), incorrect]  # Shape: (batch,)\n",
    "\n",
    "        # Calculate logit difference\n",
    "        logit_diff = correct_logits - incorrect_logits  # Shape: (batch,)\n",
    "\n",
    "        if per_prompt:\n",
    "            return logit_diff  # Return per-prompt logit differences\n",
    "        else:\n",
    "            return logit_diff.mean()  # Return mean logit difference over the batch\n",
    "\n",
    "    original_per_prompt_diff = logits_to_ave_logit_diff(linreg_logits, token_pair, per_prompt=True)\n",
    "    logger.debug(f\"Per prompt logit difference for comparison '{token_pairs_names[i]}': {original_per_prompt_diff}\")\n",
    "    original_average_logit_diff = logits_to_ave_logit_diff(linreg_logits, token_pair)\n",
    "    logger.debug(f\"Average logit difference for comparison '{token_pairs_names[i]}': {original_average_logit_diff}\")\n",
    "\n",
    "    # Retrieve final residual stream\n",
    "    final_residual_stream: Float[Tensor, \"batch seq d_model\"] = linreg_cache[\"resid_post\", -1]\n",
    "    logger.debug(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "    final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "    # Compute residual directions\n",
    "    pair_residual_directions = model.tokens_to_residual_directions(token_pair.to('cpu'))  # [batch 2 d_model]\n",
    "    logger.debug(f\"Answer residual directions shape: {pair_residual_directions.shape}\")\n",
    "\n",
    "    correct_residual_directions, incorrect_residual_directions = pair_residual_directions.unbind(dim=1)\n",
    "    logit_diff_directions = correct_residual_directions - incorrect_residual_directions  # [batch d_model]\n",
    "    logger.debug(f\"Logit difference directions shape: {logit_diff_directions.shape}\")\n",
    "\n",
    "    def residual_stack_to_logit_diff(\n",
    "        residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "        cache: ActivationCache,\n",
    "        logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    "    ) -> Float[Tensor, \"...\"]:\n",
    "        '''\n",
    "        Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "        stack of components in the residual stream.\n",
    "        '''\n",
    "        # Apply LayerNorm scaling (to just the final sequence position)\n",
    "        scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "\n",
    "        logit_diff_directions = logit_diff_directions.to(dtype=scaled_residual_stream.dtype)\n",
    "\n",
    "        logit_diff_directions = logit_diff_directions.to('cpu')\n",
    "\n",
    "        logger.debug(f\"Scaled residual stream shape: {scaled_residual_stream.shape}\")\n",
    "        logger.debug(f\"Logit diff directions shape: {logit_diff_directions.shape}\")\n",
    "\n",
    "        # Projection\n",
    "        batch_size = residual_stack.size(-2)\n",
    "        avg_logit_diff = einops.einsum(\n",
    "            scaled_residual_stream,\n",
    "            logit_diff_directions,\n",
    "            \"... batch d_model, batch d_model -> ...\"\n",
    "        ) / batch_size\n",
    "        return avg_logit_diff\n",
    "\n",
    "    # Verify residual stack computation\n",
    "    t.testing.assert_close(\n",
    "        residual_stack_to_logit_diff(final_token_residual_stream.to(t.float32), linreg_cache.to(t.float32)),\n",
    "        original_average_logit_diff.to(t.float32),\n",
    "        rtol=5e-3,  # Increased tolerance\n",
    "        atol=5e-3\n",
    "    )\n",
    "\n",
    "    # Accumulate residuals\n",
    "    accumulated_residual, labels = linreg_cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "    # accumulated_residual has shape (component, batch, d_model)\n",
    "\n",
    "    logit_lens_logit_diffs: Float[Tensor, \"component\"] = residual_stack_to_logit_diff(accumulated_residual, linreg_cache)\n",
    "    # Convert to half precision\n",
    "    logit_lens_logit_diffs = logit_lens_logit_diffs.half()\n",
    "\n",
    "    per_layer_residual, labels = linreg_cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "    per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, linreg_cache)\n",
    "\n",
    "    # # # Generate plot\n",
    "    line(\n",
    "        logit_lens_logit_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    line(\n",
    "        per_layer_logit_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Per Layer Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "    model = model.to(\"cpu\")\n",
    "    linreg_cache = linreg_cache.to(\"cpu\")\n",
    "    per_head_residual, labels = linreg_cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "    per_head_residual = per_head_residual.to(\"cpu\")\n",
    "    per_head_residual = einops.rearrange(\n",
    "        per_head_residual,\n",
    "        \"(layer head) ... -> layer head ...\",\n",
    "        layer=model.cfg.n_layers\n",
    "    )\n",
    "    per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, linreg_cache)\n",
    "\n",
    "    imshow(\n",
    "        per_head_logit_diffs,\n",
    "        labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "        title=\"Logit Difference From Each Head\",\n",
    "        width=600\n",
    "    )\n",
    "    # # Save the logit lens plot\n",
    "    # fig = line(\n",
    "    #     logit_lens_logit_diffs,\n",
    "    #     hovermode=\"x unified\", \n",
    "    #     title=f\"Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "    #     labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    #     xaxis_tickvals=labels,\n",
    "    #     width=800,\n",
    "    #     return_fig=True\n",
    "    # )\n",
    "    # # Save the file, creating it if it doesn't exist\n",
    "    # output_path = f\"../docs/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\"\n",
    "    # with open(output_path, 'w') as f:\n",
    "    #     fig.write_html(f)\n",
    "        # Create the plot\n",
    "    # fig = create_logit_lens_plot(\n",
    "    #     logit_lens_logit_diffs,\n",
    "    #     labels,\n",
    "    #     token_pairs_names[i]\n",
    "    # )\n",
    "    \n",
    "    # # Save just the plot HTML\n",
    "    # output_path = f\"../docs/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\"\n",
    "    # with open(output_path, 'w') as f:\n",
    "    #     f.write(fig.to_html(\n",
    "    #         full_html=False,\n",
    "    #         include_plotlyjs='cdn',\n",
    "    #         config={'displayModeBar': False}\n",
    "    #     ))\n",
    "    # plot_html = create_logit_lens_plot(\n",
    "    #     logit_lens_logit_diffs,\n",
    "    #     labels,\n",
    "    #     token_pairs_names[i]\n",
    "    # )\n",
    "    \n",
    "    # # Save just the plot HTML\n",
    "    # output_path = f\"../docs/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\"\n",
    "    # with open(output_path, 'w') as f:\n",
    "    #     f.write(plot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "model.clear_contexts()\n",
    "linreg_tokens = linreg_tokens.to('cpu')\n",
    "linreg_logits = linreg_logits.to('cpu')\n",
    "linreg_cache = linreg_cache.to('cpu')\n",
    "\n",
    "# Extract predictions across all datasets and convert to tensors\n",
    "predictions_store = {\n",
    "    predictor: t.tensor([\n",
    "        dataset[\"predictions\"][predictor] \n",
    "        for dataset in linreg_data_store\n",
    "    ], dtype=t.float32)\n",
    "    for predictor in linreg_data_store[0][\"predictions\"].keys()\n",
    "}\n",
    "\n",
    "# Print predictions as strings\n",
    "print(\"\\nPredictions:\")\n",
    "print(\"-\" * 50)\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\n{predictor_name}:\")\n",
    "    for i, pred in enumerate(predictor_tensor):\n",
    "        print(f\"  Sample {i}: {pred:.2f}\")\n",
    "\n",
    "\n",
    "# Now predictions_store is a dict where each value is a tensor of shape [batch]\n",
    "# Example:\n",
    "# {\n",
    "#     'gold': tensor([952.17, ...], dtype=torch.float32),\n",
    "#     'linear_regression': tensor([936.12, ...], dtype=torch.float32),\n",
    "#     'knn': tensor([555.85, ...], dtype=torch.float32),\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Verify the shape and contents\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\nPredictor: {predictor_name}\")\n",
    "    print(f\"Shape: {predictor_tensor.shape}\")\n",
    "    print(f\"Values: {predictor_tensor}\")\n",
    "\n",
    "    # Get the residual directions for numerical tokens\n",
    "    numerical_token_ids = t.tensor(list(numerical_tokens.values()), device=\"cpu\").expand(predictor_tensor.shape[0], -1)\n",
    "\n",
    "    print(f\"Numerical token IDs shape: {numerical_token_ids.shape}\")\n",
    "\n",
    "    def logits_to_numeric_mse(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        predicted_values: Float[Tensor, \"batch\"],\n",
    "        numerical_tokens: dict,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        '''\n",
    "        Returns weighted MSE between the logit predictions for numeric tokens and correct values.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model logits with shape [batch, seq, d_vocab]\n",
    "            predicted_values: Tensor of correct numeric values for each batch\n",
    "            numerical_tokens: Dictionary mapping string digits to token IDs\n",
    "            per_prompt: If True, return array of MSE per prompt, else return mean\n",
    "            \n",
    "        Returns:\n",
    "            If per_prompt=True: Tensor of MSE values per prompt\n",
    "            If per_prompt=False: Mean MSE across all prompts\n",
    "        '''\n",
    "        # Extract logits for final token\n",
    "        final_logits = logits[:, -1, :]  # Shape: [batch, d_vocab]\n",
    "        \n",
    "        # Get numeric token IDs and their corresponding values\n",
    "        numeric_ids = []\n",
    "        numeric_values = []\n",
    "        for digit, token_id in numerical_tokens.items():\n",
    "            try:\n",
    "                value = float(digit)\n",
    "                numeric_ids.append(token_id)\n",
    "                numeric_values.append(value)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        numeric_ids = t.tensor(numeric_ids, device=logits.device)\n",
    "        numeric_values = t.tensor(numeric_values, device=logits.device)\n",
    "        \n",
    "        # Get probabilities for numeric tokens\n",
    "        numeric_logits = final_logits[:, numeric_ids]  # Shape: [batch, num_numeric_tokens]\n",
    "        numeric_probs = t.softmax(numeric_logits, dim=-1)  # Shape: [batch, num_numeric_tokens]\n",
    "        \n",
    "        # Calculate expected value for each prompt\n",
    "        expected_values = (numeric_probs * numeric_values.unsqueeze(0)).sum(dim=-1)  # Shape: [batch]\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = (expected_values - predicted_values) ** 2  # Shape: [batch]\n",
    "        \n",
    "        if per_prompt:\n",
    "            return mse\n",
    "        else:\n",
    "            return mse.mean()\n",
    "\n",
    "    original_per_prompt_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens, per_prompt=True)\n",
    "    # print(f\"Per prompt numeric MSE for comparison '{token_pairs_names[i]}': {original_per_prompt_mse}\")\n",
    "    original_average_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens)\n",
    "    # print(f\"Average numeric MSE for comparison '{token_pairs_names[i]}': {original_average_mse}\")\n",
    "\n",
    "    # Retrieve final residual stream\n",
    "    final_residual_stream: Float[Tensor, \"batch seq d_model\"] = linreg_cache[\"resid_post\", -1]\n",
    "    logger.debug(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "    final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "    # Compute residual directions for numeric MSE\n",
    "    numeric_residual_directions = model.tokens_to_residual_directions(numerical_token_ids).to(\"cpu\")\n",
    "    logger.debug(f\"Numeric residual directions shape: {numeric_residual_directions.shape}\")\n",
    "\n",
    "    # # Calculate weighted residual directions based on predictor values\n",
    "    # weighted_residual_directions = numeric_residual_directions * predictor_tensor.unsqueeze(-1)  # [batch d_model]\n",
    "    # logger.debug(f\"Weighted residual directions shape: {weighted_residual_directions.shape}\")\n",
    "\n",
    "    def residual_stack_to_numeric_logits(\n",
    "        residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "        cache: ActivationCache,\n",
    "        numeric_tokens: dict,\n",
    "        numeric_residual_directions: Float[Tensor, \"batch num_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"... batch num_tokens\"]:\n",
    "        '''\n",
    "        Gets the logits for all numeric tokens at each layer of the residual stack.\n",
    "        \n",
    "        Args:\n",
    "            residual_stack: Stack of residual streams\n",
    "            cache: Model activation cache\n",
    "            numeric_tokens: Dictionary mapping string digits to token IDs\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of logits for each numeric token at each layer\n",
    "        '''\n",
    "        # Apply LayerNorm scaling\n",
    "        scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "        scaled_residual_stream = scaled_residual_stream.to(\"cpu\")\n",
    "        numeric_residual_directions = numeric_residual_directions.to(dtype=scaled_residual_stream.dtype) \n",
    "        \n",
    "        # Project residual stream onto numeric directions\n",
    "        numeric_logits = einops.einsum(\n",
    "            scaled_residual_stream, \n",
    "            numeric_residual_directions,\n",
    "            \"... batch d_model, batch num_tokens d_model -> ... batch num_tokens\"\n",
    "        )\n",
    "        \n",
    "        return numeric_logits\n",
    "\n",
    "    # # Verify residual stack computation\n",
    "    # t.testing.assert_close(\n",
    "    #     residual_stack_to_logit_diff(final_token_residual_stream.to(t.float32), linreg_cache.to(t.float32)),\n",
    "    #     original_average_logit_diff.to(t.float32),\n",
    "    #     rtol=5e-3,  # Increased tolerance\n",
    "    #     atol=5e-3\n",
    "    # )\n",
    "\n",
    "    # Get accumulated residuals across layers\n",
    "    accumulated_residual, labels = linreg_cache.accumulated_resid(\n",
    "        layer=-1, \n",
    "        incl_mid=True, \n",
    "        pos_slice=-1, \n",
    "        return_labels=True\n",
    "    )\n",
    "\n",
    "    # Get logits for numeric tokens at each layer\n",
    "    numeric_logits = residual_stack_to_numeric_logits(\n",
    "        accumulated_residual,\n",
    "        linreg_cache,\n",
    "        numerical_tokens,\n",
    "        numeric_residual_directions\n",
    "    )\n",
    "\n",
    "    # Print shape of numeric_logits tensor\n",
    "    print(\"Shape of numeric_logits:\", numeric_logits.shape)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    numeric_probs = t.softmax(numeric_logits, dim=-1)  # [..., batch, num_tokens]\n",
    "\n",
    "    print(\"Shape of numeric_logits:\", numeric_probs.shape)\n",
    "    print(\"Size of numerical tokens:\", len(numerical_tokens))\n",
    "\n",
    "\n",
    "    # Calculate expected numeric value at each layer\n",
    "    # Filter out non-numeric tokens and create numeric_values tensor\n",
    "    numeric_values = []\n",
    "    for digit in numerical_tokens.keys():\n",
    "        try:\n",
    "            # Handle both integer digits and floating point numbers\n",
    "            if isinstance(digit, (int, float)):\n",
    "                value = float(digit)\n",
    "            else:\n",
    "                value = float(digit.replace(',', ''))  # Handle any commas in number strings\n",
    "            numeric_values.append(value)\n",
    "        except ValueError as e:\n",
    "            print(f\"ERROR converting '{digit}' to float:\", e)\n",
    "            continue\n",
    "    numeric_values = t.tensor(numeric_values, device=numeric_probs.device)\n",
    "\n",
    "    # Reshape numeric_values for broadcasting\n",
    "    numeric_values = numeric_values.view(1, 1, -1)  # Shape: [1, 1, num_tokens]\n",
    "\n",
    "    # Calculate expected values with broadcasting\n",
    "    expected_values = (numeric_probs * numeric_values).sum(dim=-1)  # [..., batch]\n",
    "\n",
    "    # Plot how the predicted value evolves across layers\n",
    "    line(\n",
    "        expected_values.mean(dim=1),  # Average across batch\n",
    "        hovermode=\"x unified\",\n",
    "        title=\"Evolution of Numeric Prediction Across Layers\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Predicted Value\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    # Also plot the confidence (entropy) in the prediction\n",
    "    entropy = -(numeric_probs * t.log(numeric_probs)).sum(dim=-1)  # [..., batch]\n",
    "    line(\n",
    "        entropy.mean(dim=1),  # Average across batch\n",
    "        hovermode=\"x unified\",\n",
    "        title=\"Evolution of Prediction Confidence Across Layers\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Entropy (lower = more confident)\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "    # # Save the logit lens plot\n",
    "    # fig = line(\n",
    "    #     logit_lens_logit_diffs,\n",
    "    #     hovermode=\"x unified\", \n",
    "    #     title=f\"Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "    #     labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    #     xaxis_tickvals=labels,\n",
    "    #     width=800,\n",
    "    #     return_fig=True\n",
    "    # )\n",
    "    # # Save the file, creating it if it doesn't exist\n",
    "    # output_path = f\"../docs/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\"\n",
    "    # with open(output_path, 'w') as f:\n",
    "    #     fig.write_html(f)\n",
    "        # Create the plot\n",
    "    # fig = create_logit_lens_plot(\n",
    "    #     logit_lens_logit_diffs,\n",
    "    #     labels,\n",
    "    #     token_pairs_names[i]\n",
    "    # )\n",
    "    \n",
    "    # # Save just the plot HTML\n",
    "    # output_path = f\"../docs/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\"\n",
    "    # with open(output_path, 'w') as f:\n",
    "    #     f.write(fig.to_html(\n",
    "    #         full_html=False,\n",
    "    #         include_plotlyjs='cdn',\n",
    "    #         config={'displayModeBar': False}\n",
    "    #     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_hunter.model_utils import get_numerical_tokens\n",
    "\n",
    "numerical_tokens = get_numerical_tokens(model)\n",
    "\n",
    "model.clear_contexts()\n",
    "linreg_tokens = linreg_tokens.to('cpu')\n",
    "linreg_logits = linreg_logits.to('cpu')\n",
    "linreg_cache = linreg_cache.to('cpu')\n",
    "\n",
    "# Extract predictions across all datasets and convert to tensors\n",
    "predictions_store = {\n",
    "    predictor: t.tensor([\n",
    "        dataset[\"predictions\"][predictor] \n",
    "        for dataset in linreg_data_store\n",
    "    ], dtype=t.float32)\n",
    "    for predictor in linreg_data_store[0][\"predictions\"].keys()\n",
    "}\n",
    "\n",
    "# Print predictions as strings\n",
    "print(\"\\nPredictions:\")\n",
    "print(\"-\" * 50)\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\n{predictor_name}:\")\n",
    "    for i, pred in enumerate(predictor_tensor):\n",
    "        print(f\"  Sample {i}: {pred:.2f}\")\n",
    "\n",
    "\n",
    "# Now predictions_store is a dict where each value is a tensor of shape [batch]\n",
    "# Example:\n",
    "# {\n",
    "#     'gold': tensor([952.17, ...], dtype=torch.float32),\n",
    "#     'linear_regression': tensor([936.12, ...], dtype=torch.float32),\n",
    "#     'knn': tensor([555.85, ...], dtype=torch.float32),\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Verify the shape and contents\n",
    "for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    print(f\"\\nPredictor: {predictor_name}\")\n",
    "    print(f\"Shape: {predictor_tensor.shape}\")\n",
    "    print(f\"Values: {predictor_tensor}\")\n",
    "\n",
    "    # Get the residual directions for numerical tokens\n",
    "    numerical_token_ids = t.tensor(list(numerical_tokens.values()), device=\"cpu\").expand(predictor_tensor.shape[0], -1)\n",
    "\n",
    "    print(f\"Numerical token IDs shape: {numerical_token_ids.shape}\")\n",
    "\n",
    "    def logits_to_numeric_mse(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        predicted_values: Float[Tensor, \"batch\"],\n",
    "        numerical_tokens: dict,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        '''\n",
    "        Returns weighted MSE between the logit predictions for numeric tokens and correct values.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model logits with shape [batch, seq, d_vocab]\n",
    "            predicted_values: Tensor of correct numeric values for each batch\n",
    "            numerical_tokens: Dictionary mapping string digits to token IDs\n",
    "            per_prompt: If True, return array of MSE per prompt, else return mean\n",
    "            \n",
    "        Returns:\n",
    "            If per_prompt=True: Tensor of MSE values per prompt\n",
    "            If per_prompt=False: Mean MSE across all prompts\n",
    "        '''\n",
    "        # Extract logits for final token\n",
    "        final_logits = logits[:, -1, :]  # Shape: [batch, d_vocab]\n",
    "        \n",
    "        # Get numeric token IDs and their corresponding values\n",
    "        numeric_ids = []\n",
    "        numeric_values = []\n",
    "        for digit, token_id in numerical_tokens.items():\n",
    "            try:\n",
    "                value = float(digit)\n",
    "                numeric_ids.append(token_id)\n",
    "                numeric_values.append(value)\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        numeric_ids = t.tensor(numeric_ids, device=logits.device)\n",
    "        numeric_values = t.tensor(numeric_values, device=logits.device)\n",
    "        \n",
    "        # Get probabilities for numeric tokens\n",
    "        numeric_logits = final_logits[:, numeric_ids]  # Shape: [batch, num_numeric_tokens]\n",
    "        numeric_probs = t.softmax(numeric_logits, dim=-1)  # Shape: [batch, num_numeric_tokens]\n",
    "        \n",
    "        # Calculate expected value for each prompt\n",
    "        expected_values = (numeric_probs * numeric_values.unsqueeze(0)).sum(dim=-1)  # Shape: [batch]\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = (expected_values - predicted_values) ** 2  # Shape: [batch]\n",
    "        \n",
    "        if per_prompt:\n",
    "            return mse\n",
    "        else:\n",
    "            return mse.mean()\n",
    "\n",
    "    original_per_prompt_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens, per_prompt=True)\n",
    "    # print(f\"Per prompt numeric MSE for comparison '{token_pairs_names[i]}': {original_per_prompt_mse}\")\n",
    "    original_average_mse = logits_to_numeric_mse(linreg_logits, predictor_tensor, numerical_tokens)\n",
    "    # print(f\"Average numeric MSE for comparison '{token_pairs_names[i]}': {original_average_mse}\")\n",
    "\n",
    "    # Retrieve final residual stream\n",
    "    final_residual_stream: Float[Tensor, \"batch seq d_model\"] = linreg_cache[\"resid_post\", -1]\n",
    "    logger.debug(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "    final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "    # Compute residual directions for numeric MSE\n",
    "    numeric_residual_directions = model.tokens_to_residual_directions(numerical_token_ids).to(\"cpu\")\n",
    "    logger.debug(f\"Numeric residual directions shape: {numeric_residual_directions.shape}\")\n",
    "\n",
    "    # # Calculate weighted residual directions based on predictor values\n",
    "    # weighted_residual_directions = numeric_residual_directions * predictor_tensor.unsqueeze(-1)  # [batch d_model]\n",
    "    # logger.debug(f\"Weighted residual directions shape: {weighted_residual_directions.shape}\")\n",
    "\n",
    "    def residual_stack_to_numeric_logits(\n",
    "        residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "        cache: ActivationCache,\n",
    "        numeric_tokens: dict,\n",
    "        numeric_residual_directions: Float[Tensor, \"batch num_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"... batch num_tokens\"]:\n",
    "        '''\n",
    "        Gets the logits for all numeric tokens at each layer of the residual stack.\n",
    "        \n",
    "        Args:\n",
    "            residual_stack: Stack of residual streams\n",
    "            cache: Model activation cache\n",
    "            numeric_tokens: Dictionary mapping string digits to token IDs\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of logits for each numeric token at each layer\n",
    "        '''\n",
    "        # Apply LayerNorm scaling\n",
    "        scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "        scaled_residual_stream = scaled_residual_stream.to(\"cpu\")\n",
    "        numeric_residual_directions = numeric_residual_directions.to(dtype=scaled_residual_stream.dtype) \n",
    "        \n",
    "        # Project residual stream onto numeric directions\n",
    "        numeric_logits = einops.einsum(\n",
    "            scaled_residual_stream, \n",
    "            numeric_residual_directions,\n",
    "            \"... batch d_model, batch num_tokens d_model -> ... batch num_tokens\"\n",
    "        )\n",
    "        \n",
    "        return numeric_logits\n",
    "    ###############################################################################\n",
    "    # Within your for predictor_name, predictor_tensor in predictions_store.items():\n",
    "    ###############################################################################\n",
    "\n",
    "    # 1) Retrieve accumulated residuals for all layers (including mid-layer if desired)\n",
    "    accumulated_residual, labels = linreg_cache.accumulated_resid(\n",
    "        layer=-1, \n",
    "        incl_mid=True,      # Keep mid-layer positions if you want finer granularity\n",
    "        pos_slice=-1,       # Only final token in each prompt\n",
    "        return_labels=True\n",
    "    )\n",
    "    # accumulated_residual has shape [num_layers_plus_mid, batch, d_model]\n",
    "\n",
    "    # 2) Project the residual at each layer onto numeric token directions\n",
    "    #    We'll define a helper function below called get_numeric_logits_per_layer\n",
    "    def get_numeric_logits_per_layer(\n",
    "        residual_stack: Float[Tensor, \"layers batch d_model\"],\n",
    "        cache: ActivationCache,\n",
    "        numeric_residual_directions: Float[Tensor, \"batch num_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"layers batch num_tokens\"]:\n",
    "        \"\"\"\n",
    "        For each layer in residual_stack, apply the final LN to get the correct scaling,\n",
    "        then dot with numeric_residual_directions to get numeric logits.\n",
    "        \n",
    "        Args:\n",
    "            residual_stack: shape [layers, batch, d_model]\n",
    "            cache: ActivationCache to apply LN\n",
    "            numeric_residual_directions: shape [batch, num_tokens, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            numeric_logits: shape [layers, batch, num_tokens]\n",
    "        \"\"\"\n",
    "        # Apply LN to each layer's [batch, d_model]\n",
    "        # apply_ln_to_stack expects shape [layers, batch, d_model],\n",
    "        # returns the same shape after LN.\n",
    "        scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "        scaled_residual_stream = scaled_residual_stream.to(\"cpu\")\n",
    "\n",
    "        # Make sure directions have the same dtype\n",
    "        numeric_residual_directions = numeric_residual_directions.to(dtype=scaled_residual_stream.dtype)\n",
    "\n",
    "        # Dot product to get logits\n",
    "        # scaled_residual_stream: [layers, batch, d_model]\n",
    "        # numeric_residual_directions: [batch, num_tokens, d_model]\n",
    "        # We'll broadcast batch dimension. We'll need einops or explicit broadcasting:\n",
    "        numeric_logits = einops.einsum(\n",
    "            scaled_residual_stream,             # [layers, batch, d_model]\n",
    "            numeric_residual_directions,        # [batch, num_tokens, d_model]\n",
    "            \"layer b d, b t d -> layer b t\"\n",
    "        )\n",
    "        return numeric_logits\n",
    "\n",
    "    numeric_logits_per_layer = get_numeric_logits_per_layer(\n",
    "        accumulated_residual,\n",
    "        linreg_cache,\n",
    "        numeric_residual_directions\n",
    "    )\n",
    "    # numeric_logits_per_layer: [num_layers_plus_mid, batch, num_tokens]\n",
    "    print(\"Shape of numeric_logits_per_layer:\", numeric_logits_per_layer.shape)\n",
    "\n",
    "    # 3) Convert to probabilities per layer\n",
    "    numeric_probs_per_layer = t.softmax(numeric_logits_per_layer, dim=-1)  # [layers, batch, num_tokens]\n",
    "    print(\"Shape of numeric_probs_per_layer:\", numeric_probs_per_layer.shape)\n",
    "\n",
    "    # 4) Build numeric_values Tensor from your dictionary keys\n",
    "    numeric_values_list = []\n",
    "    for digit in numerical_tokens.keys():\n",
    "        try:\n",
    "            numeric_values_list.append(float(digit))\n",
    "        except ValueError:\n",
    "            pass  # skip if it can't convert\n",
    "    numeric_values_tensor = t.tensor(numeric_values_list, device=numeric_probs_per_layer.device)\n",
    "\n",
    "    # 5) Compute expected numeric value per layer: E[number]\n",
    "    # numeric_probs_per_layer: [layers, batch, num_tokens]\n",
    "    # numeric_values_tensor: [num_tokens]\n",
    "    # We'll reshape numeric_values_tensor to broadcast\n",
    "    numeric_values_reshaped = numeric_values_tensor.view(1, 1, -1)  # [1, 1, num_tokens]\n",
    "\n",
    "    # Multiply + sum over num_tokens axis\n",
    "    # We'll get [layers, batch]\n",
    "    expected_values_per_layer = (numeric_probs_per_layer * numeric_values_reshaped).sum(dim=-1)\n",
    "\n",
    "    # 6) Compute MSE at each layer. predictor_tensor has shape [batch], the \"true\" or gold values.\n",
    "    # We'll broadcast or expand to [layers, batch] so we can do (pred - gold)^2\n",
    "    # If you want to treat predictor_tensor as the \"gold,\" do so:\n",
    "    gold_values = predictor_tensor  # shape [batch]\n",
    "    gold_values_expanded = gold_values.unsqueeze(0)  # shape [1, batch]\n",
    "\n",
    "    layerwise_mse = (expected_values_per_layer - gold_values_expanded) ** 2  # [layers, batch]\n",
    "    # Average across the batch dimension\n",
    "    layerwise_mse_mean = layerwise_mse.mean(dim=-1)  # [layers]\n",
    "\n",
    "    print(f\"Layerwise MSE shape: {layerwise_mse_mean.shape}\")\n",
    "\n",
    "    # 7) Plot how the MSE evolves across layers\n",
    "    line(\n",
    "        layerwise_mse_mean,  # shape [layers]\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"MSE vs. Gold Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"MSE\"},\n",
    "        xaxis_tickvals=labels,  # `labels` from accumulated_resid\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    #########################################################################\n",
    "    # (Optional) Also plot how the expected numeric value changes across layers\n",
    "    #########################################################################\n",
    "\n",
    "    avg_expected_across_batch = expected_values_per_layer.mean(dim=-1)  # [layers]\n",
    "    line(\n",
    "        avg_expected_across_batch,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Expected Numeric Prediction Across Layers for {predictor_name}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Predicted Value\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e66d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
