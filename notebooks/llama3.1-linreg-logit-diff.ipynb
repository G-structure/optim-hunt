{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "from optim_hunter.plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference, line\n",
    "from optim_hunter.utils import prepare_prompt, slice_dataset\n",
    "from optim_hunter.sklearn_regressors import linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random\n",
    "from optim_hunter.datasets import get_dataset_friedman_2\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "â€ºlogger = logging.getLogger(__name__)\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "#device = t.device(\"cuda:0,1\" if t.cuda.is_available() else \"cpu\")\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# device = t.device(\"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directly from model path https://github.com/TransformerLensOrg/TransformerLens/issues/691\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "MODEL_PATH = \"/home/freiza/optim_hunter/.models/Llama-3.1-8B-Instruct/\"\n",
    "\n",
    "if MODEL_PATH:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True,\n",
    "                                                     #quantization_config=BitsAndBytesConfig(load_in_4bit=True), \n",
    "                                                     #torch_dtype = t.float32, \n",
    "                                                     #device_map = \"cuda:0\"\n",
    "                                                     )\n",
    "\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        MODEL_TYPE,\n",
    "        hf_model=hf_model,\n",
    "        device=\"cuda\",\n",
    "        n_devices=2,\n",
    "        fold_ln=True,\n",
    "        # fold_value_biases=False,\n",
    "        center_writing_weights=True,\n",
    "        # refactor_factored_attn_matrices=True,\n",
    "        center_unembed=True,\n",
    "        # dtype=t.bfloat16,\n",
    "        dtype=t.float16,\n",
    "        default_padding_side='left',\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    #model = model.to(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "    #model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65997326",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "x_train, y_train, x_test, y_test = get_dataset_friedman_2(random_state=11)\n",
    "# x_train, y_train, x_test, y_test =  slice_dataset(x_train, y_train, x_test, y_test, seq_len)\n",
    "linear_regression_prediction = linear_regression(x_train, x_test, y_train, y_test)['y_predict']\n",
    "knn_regression_prediction = knn_regression(x_train, x_test, y_train, y_test)['y_predict']\n",
    "random_forest_prediction = random_forest(x_train, x_test, y_train, y_test)['y_predict']\n",
    "baseline_avg_prediction = baseline_average(x_train, x_test, y_train, y_test)['y_predict']\n",
    "baseline_last_prediction = baseline_last(x_train, x_test, y_train, y_test)['y_predict']\n",
    "baseline_random_prediction = baseline_random(x_train, x_test, y_train, y_test)['y_predict']\n",
    "gold = y_test.values[0]\n",
    "\n",
    "print(\"Linear Regression Prediction:\", linear_regression_prediction[0])\n",
    "print(\"Random Forest Prediction:\", random_forest_prediction[0])\n",
    "print(\"KNN Regression Prediction:\", knn_regression_prediction[0])\n",
    "print(\"Baseline Average Prediction:\", baseline_avg_prediction[0])\n",
    "print(\"Baseline Last Prediction:\", baseline_last_prediction[0]) \n",
    "print(\"Baseline Random Prediction:\", baseline_random_prediction[0])\n",
    "print(\"Gold:\", gold)\n",
    "\n",
    "prompt = prepare_prompt(x_train, y_train, x_test)\n",
    "prompt = prompt + \"\"\n",
    "# example_answer = f\"{baseline_last_prediction[0]}\"\n",
    "# example_answer = f\"{random_forest_prediction[0]}\"\n",
    "example_answer = f\"{gold}\"\n",
    "utils.test_prompt(prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f361e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_data(model, dataset_func, regressors, random_state=1, seq_len=None):\n",
    "    \"\"\"\n",
    "    Creates a structured comparison dataset for analyzing different regression models against gold values.\n",
    "    \n",
    "    Args:\n",
    "        model (HookedTransformer): The transformer model used for tokenization\n",
    "        dataset_func (callable): Function that returns (x_train, y_train, x_test, y_test)\n",
    "        regressors (list): List of regression functions to compare\n",
    "        random_state (int, optional): Random seed for dataset generation. Defaults to 11\n",
    "    \n",
    "    Returns:\n",
    "        dict: A structured dictionary containing:\n",
    "        {\n",
    "            'dataset_name': str,  # Name of the dataset function\n",
    "            'prompt': str,        # Generated prompt text for the model\n",
    "            'predictions': {      # Dictionary of predictions from each model\n",
    "                'gold': float,    # True value\n",
    "                'model_name1': float,  # Prediction from first model\n",
    "                'model_name2': float,  # Prediction from second model\n",
    "                ...\n",
    "            },\n",
    "            'comparison_names': [  # List of comparison descriptors\n",
    "                'model1 vs model2',\n",
    "                'model1 vs model3',\n",
    "                ...\n",
    "            ],\n",
    "            'token_pairs': tensor  # Shape: [num_comparisons, 1, 2]\n",
    "                                  # Each pair contains the first tokens of two predictions\n",
    "                                  # being compared\n",
    "        }\n",
    "    \n",
    "    Note:\n",
    "        - The function generates unique combinations (not permutations) of comparisons\n",
    "        - Only the first token of each prediction is stored in token_pairs\n",
    "        - All possible combinations between gold and regressors are included\n",
    "        - Token pairs maintain the order specified in comparison_names\n",
    "    \"\"\"\n",
    "    # Get dataset\n",
    "    x_train, y_train, x_test, y_test = dataset_func(random_state=random_state)\n",
    "    if seq_len:\n",
    "        x_train, y_train, x_test, y_test = slice_dataset(x_train, y_train, x_test, y_test, seq_len)\n",
    "    \n",
    "    # Get prompt\n",
    "    prompt = prepare_prompt(x_train, y_train, x_test)\n",
    "    \n",
    "    # Get gold value\n",
    "    gold = y_test.values[0]\n",
    "    \n",
    "    # Get predictions from each regressor\n",
    "    predictions = {}\n",
    "    predictions['gold'] = gold\n",
    "    for regressor in regressors:\n",
    "        result = regressor(x_train, x_test, y_train, y_test)\n",
    "        predictions[result['model_name']] = result['y_predict'][0]\n",
    "    \n",
    "    # Create comparison names and token pairs\n",
    "    comparison_names = []\n",
    "    token_pairs = []\n",
    "    \n",
    "    # Create list of all predictors (including gold)\n",
    "    all_predictors = ['gold'] + [reg(x_train, x_test, y_train, y_test)['model_name'] for reg in regressors]\n",
    "    \n",
    "    # Generate unique combinations (not permutations)\n",
    "    for i, pred1 in enumerate(all_predictors):\n",
    "        for j, pred2 in enumerate(all_predictors[i+1:], i+1):  # Start from i+1 to avoid duplicates\n",
    "            comparison_name = f\"{pred1} vs {pred2}\"\n",
    "            comparison_names.append(comparison_name)\n",
    "            \n",
    "            # Tokenize each prediction separately and get their first tokens\n",
    "            tokens1 = model.to_tokens(str(predictions[pred1]), prepend_bos=False)[0, 0]  # First token of first prediction\n",
    "            tokens2 = model.to_tokens(str(predictions[pred2]), prepend_bos=False)[0, 0]  # First token of second prediction\n",
    "            \n",
    "            # Combine the first tokens into a pair\n",
    "            first_tokens = t.tensor([tokens1, tokens2], device=tokens1.device).unsqueeze(0)  # Shape: [1, 2]\n",
    "            token_pairs.append(first_tokens)\n",
    "\n",
    "    # Verification Step: Ensure that each comparison_name matches the corresponding token_pair\n",
    "    for idx, (comp_name, token_pair) in enumerate(zip(comparison_names, token_pairs)):\n",
    "        pred1_name, pred2_name = comp_name.split(' vs ')\n",
    "        pred1_value = predictions[pred1_name]\n",
    "        pred2_value = predictions[pred2_name]\n",
    "        \n",
    "        # Tokenize the actual prediction values\n",
    "        actual_tokens1 = model.to_tokens(str(pred1_value), prepend_bos=False)[0, 0].item()\n",
    "        actual_tokens2 = model.to_tokens(str(pred2_value), prepend_bos=False)[0, 0].item()\n",
    "        \n",
    "        # Extract tokens from token_pair\n",
    "        token1, token2 = token_pair.squeeze(0).tolist()\n",
    "        \n",
    "        # Assert that tokens match\n",
    "        assert token1 == actual_tokens1, f\"Mismatch in token1 for comparison '{comp_name}' at index {idx}\"\n",
    "        assert token2 == actual_tokens2, f\"Mismatch in token2 for comparison '{comp_name}' at index {idx}\"\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'dataset_name': dataset_func.__name__,\n",
    "        'prompt': prompt,\n",
    "        'predictions': predictions,\n",
    "        'comparison_names': comparison_names,\n",
    "        'token_pairs': t.stack(token_pairs),  # Shape: [num_comparisons, 1, 2]\n",
    "    }\n",
    "\n",
    "# # Create the data store\n",
    "# datasets = [ get_dataset_friedman_2 ]\n",
    "# regressors = [ linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random ]\n",
    "# data_store = {}\n",
    "# for dataset_func in datasets:\n",
    "#     data_store[dataset_func.__name__] = create_comparison_data(model, dataset_func, regressors)\n",
    "# # Print out the token pairs and comparison names\n",
    "# for dataset_name, dataset_info in data_store.items():\n",
    "#     print(f\"\\nDataset: {dataset_name}\")\n",
    "#     print(\"Token pairs:\")\n",
    "#     print(dataset_info['token_pairs'])\n",
    "#     print(\"\\nComparison names:\")\n",
    "#     print(dataset_info['comparison_names'])\n",
    "\n",
    "# # Get the first token pair from the first dataset\n",
    "# first_dataset_name = next(iter(data_store))\n",
    "# first_token_pair = data_store[first_dataset_name]['token_pairs'][0]  # Shape: [1, 2]\n",
    "# print(\"\\nFirst token pair:\")\n",
    "# print(first_token_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91e6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [ linear_regression, knn_regression, random_forest, baseline_average, baseline_last, baseline_random ]\n",
    "\n",
    "def generate_linreg_tokens(\n",
    "    model: HookedTransformer,\n",
    "    dataset,\n",
    "    seq_len = 5,\n",
    "    batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    '''\n",
    "    Generates a sequence of linear regression ICL tokens\n",
    "\n",
    "    Outputs are:\n",
    "        linreg_tokens: [batch, 1+linreg]\n",
    "    '''\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long().to(device)\n",
    "    zero_token = model.to_tokens('0', truncate=True)[0][-1]\n",
    "    \n",
    "    # Create list to store tokens for each batch\n",
    "    batch_tokens = []\n",
    "    data_store = []\n",
    "\n",
    "    dataset_func = get_dataset_friedman_2\n",
    "    \n",
    "    # Generate tokens for each batch with different random seeds\n",
    "    for i in range(batch):\n",
    "        data = create_comparison_data(model, dataset_func, regressors, random_state=i, seq_len=seq_len)\n",
    "        tokens = model.to_tokens(data['prompt'], truncate=True)\n",
    "        batch_tokens.append(tokens[0])\n",
    "        data_store.append(data)\n",
    "    \n",
    "    # Find the longest sequence length\n",
    "    max_len = max(len(tokens) for tokens in batch_tokens)\n",
    "    \n",
    "    # Pad shorter sequences with token 0 at position -4\n",
    "    for i in range(len(batch_tokens)):\n",
    "        while len(batch_tokens[i]) < max_len:\n",
    "            # Insert 0 at position -4 from the end\n",
    "            print(f\"Found mismatch in token length for batch {i}!\\nLargest length: {max_len}\\nBatch {i} length: {len(batch_tokens[i])}\\nApplying padding...\")\n",
    "            print(f\"\\nBefore Zero Token Padding:\\n#####\\n{model.to_string(batch_tokens[i][-50:])}\\n#####\")\n",
    "            batch_tokens[i] = t.cat([\n",
    "                batch_tokens[i][:len(batch_tokens[i])-3],  \n",
    "                zero_token.unsqueeze(0), # Add unsqueeze to make zero_token 1-dimensional\n",
    "                batch_tokens[i][len(batch_tokens[i])-3:]\n",
    "            ])\n",
    "            print(f\"\\nAfter Zero Token Padding:\\n#####\\n{model.to_string(batch_tokens[i][-50:])}\\n#####\")\n",
    "\n",
    "    \n",
    "    # Stack all batches together \n",
    "    linreg_tokens = t.stack(batch_tokens).to(device)\n",
    "    \n",
    "    # Add prefix to each batch\n",
    "    linreg_tokens = t.cat([prefix, linreg_tokens], dim=-1).to(device)\n",
    "    return linreg_tokens, data_store\n",
    "\n",
    "def run_and_cache_model_linreg_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "    '''\n",
    "    Generates a sequence of linear regression ICL tokens, and runs the model on it, returning (tokens, logits, cache)\n",
    "\n",
    "    Should use the `generate_linreg_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        linreg_tokens: [batch, 1+linreg]\n",
    "        linreg_logits: [batch, 1+linreg, d_vocab]\n",
    "        linreg_cache: The cache of the model run on linreg_tokens\n",
    "    '''\n",
    "    linreg_tokens, linreg_data_store = generate_linreg_tokens(model, get_dataset_friedman_2, seq_len, batch)\n",
    "    linreg_logits, linreg_cache = model.run_with_cache(linreg_tokens)\n",
    "    return linreg_tokens, linreg_logits, linreg_cache, linreg_data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_contexts()\n",
    "\n",
    "seq_len = 10\n",
    "batch = 4\n",
    "(linreg_tokens, linreg_logits, linreg_cache, linreg_data_store) = run_and_cache_model_linreg_tokens(model, seq_len, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_contexts()\n",
    "linreg_tokens = linreg_tokens.to('cpu')\n",
    "linreg_logits = linreg_logits.to('cpu')\n",
    "linreg_cache = linreg_cache.to('cpu')\n",
    "\n",
    "# Verify all datasets have the same comparison names\n",
    "base_comparison_names = linreg_data_store[0][\"comparison_names\"]\n",
    "all_match = all(dataset[\"comparison_names\"] == base_comparison_names for dataset in linreg_data_store[1:])\n",
    "assert all_match, \"Mismatch in comparison names across datasets.\"\n",
    "\n",
    "# Extract comparison names from the first dataset\n",
    "token_pairs_names = base_comparison_names.copy()\n",
    "\n",
    "# Extract token pairs across all datasets for each comparison\n",
    "token_pairs = [\n",
    "    t.stack([dataset[\"token_pairs\"][i] for dataset in linreg_data_store])[0]\n",
    "    for i in range(len(token_pairs_names))\n",
    "]\n",
    "\n",
    "logger.info(f\"Number of comparisons: {len(token_pairs_names)}\")\n",
    "logger.info(f\"Number of token_pairs: {len(token_pairs)}\")\n",
    "\n",
    "# Iterate over token pairs and generate plots\n",
    "for i, token_pair in enumerate(token_pairs):\n",
    "    logger.info(f\"Processing comparison {i}: {token_pairs_names[i]}\")\n",
    "    token_pair = token_pair.to('cpu')\n",
    "\n",
    "    def logits_to_ave_logit_diff(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        answer_tokens: Float[Tensor, \"batch 2\"] = token_pair,\n",
    "        per_prompt: bool = False\n",
    "    ) -> Float[Tensor, \"*batch\"]:\n",
    "        '''\n",
    "        Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "        If per_prompt=True, return the array of differences rather than the average.\n",
    "        '''\n",
    "        # Extract token IDs for correct and incorrect answers\n",
    "        correct = answer_tokens[:, 0]  # Correct token IDs\n",
    "        incorrect = answer_tokens[:, 1]  # Incorrect token IDs\n",
    "\n",
    "        # Extract logits for the final token in the sequence\n",
    "        final_logits = logits[:, -1, :]  # Shape: (batch, d_vocab)\n",
    "\n",
    "        # Get logits for the correct and incorrect answers\n",
    "        correct_logits = final_logits[t.arange(final_logits.size(0)), correct]  # Shape: (batch,)\n",
    "        incorrect_logits = final_logits[t.arange(final_logits.size(0)), incorrect]  # Shape: (batch,)\n",
    "\n",
    "        # Calculate logit difference\n",
    "        logit_diff = correct_logits - incorrect_logits  # Shape: (batch,)\n",
    "\n",
    "        if per_prompt:\n",
    "            return logit_diff  # Return per-prompt logit differences\n",
    "        else:\n",
    "            return logit_diff.mean()  # Return mean logit difference over the batch\n",
    "\n",
    "    original_per_prompt_diff = logits_to_ave_logit_diff(linreg_logits, token_pair, per_prompt=True)\n",
    "    logger.debug(f\"Per prompt logit difference for comparison '{token_pairs_names[i]}': {original_per_prompt_diff}\")\n",
    "    original_average_logit_diff = logits_to_ave_logit_diff(linreg_logits, token_pair)\n",
    "    logger.debug(f\"Average logit difference for comparison '{token_pairs_names[i]}': {original_average_logit_diff}\")\n",
    "\n",
    "    # Retrieve final residual stream\n",
    "    final_residual_stream: Float[Tensor, \"batch seq d_model\"] = linreg_cache[\"resid_post\", -1]\n",
    "    logger.debug(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "    final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "    # Compute residual directions\n",
    "    pair_residual_directions = model.tokens_to_residual_directions(token_pair.to('cpu'))  # [batch 2 d_model]\n",
    "    logger.debug(f\"Answer residual directions shape: {pair_residual_directions.shape}\")\n",
    "\n",
    "    correct_residual_directions, incorrect_residual_directions = pair_residual_directions.unbind(dim=1)\n",
    "    logit_diff_directions = correct_residual_directions - incorrect_residual_directions  # [batch d_model]\n",
    "    logger.debug(f\"Logit difference directions shape: {logit_diff_directions.shape}\")\n",
    "\n",
    "    def residual_stack_to_logit_diff(\n",
    "        residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "        cache: ActivationCache,\n",
    "        logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    "    ) -> Float[Tensor, \"...\"]:\n",
    "        '''\n",
    "        Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "        stack of components in the residual stream.\n",
    "        '''\n",
    "        # Apply LayerNorm scaling (to just the final sequence position)\n",
    "        scaled_residual_stream = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "\n",
    "        logit_diff_directions = logit_diff_directions.to(dtype=scaled_residual_stream.dtype)\n",
    "\n",
    "        logit_diff_directions = logit_diff_directions.to('cpu')\n",
    "\n",
    "        logger.debug(f\"Scaled residual stream shape: {scaled_residual_stream.shape}\")\n",
    "        logger.debug(f\"Logit diff directions shape: {logit_diff_directions.shape}\")\n",
    "\n",
    "        # Projection\n",
    "        batch_size = residual_stack.size(-2)\n",
    "        avg_logit_diff = einops.einsum(\n",
    "            scaled_residual_stream,\n",
    "            logit_diff_directions,\n",
    "            \"... batch d_model, batch d_model -> ...\"\n",
    "        ) / batch_size\n",
    "        return avg_logit_diff\n",
    "\n",
    "    # Verify residual stack computation\n",
    "    t.testing.assert_close(\n",
    "        residual_stack_to_logit_diff(final_token_residual_stream.to(t.float32), linreg_cache.to(t.float32)),\n",
    "        original_average_logit_diff.to(t.float32),\n",
    "        rtol=5e-3,  # Increased tolerance\n",
    "        atol=5e-3\n",
    "    )\n",
    "\n",
    "    # Accumulate residuals\n",
    "    accumulated_residual, labels = linreg_cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "    # accumulated_residual has shape (component, batch, d_model)\n",
    "\n",
    "    logit_lens_logit_diffs: Float[Tensor, \"component\"] = residual_stack_to_logit_diff(accumulated_residual, linreg_cache)\n",
    "    # Convert to half precision\n",
    "    logit_lens_logit_diffs = logit_lens_logit_diffs.half()\n",
    "\n",
    "    # # Generate plot\n",
    "    line(\n",
    "        logit_lens_logit_diffs,\n",
    "        hovermode=\"x unified\",\n",
    "        title=f\"Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800\n",
    "    )\n",
    "    # Save the logit lens plot\n",
    "    fig = line(\n",
    "        logit_lens_logit_diffs,\n",
    "        hovermode=\"x unified\", \n",
    "        title=f\"Logit Difference From Accumulated Residual Stream for {token_pairs_names[i]}\",\n",
    "        labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "        xaxis_tickvals=labels,\n",
    "        width=800,\n",
    "        return_fig=True\n",
    "    )\n",
    "    fig.write_html(f\"../public/logit_lens_{token_pairs_names[i].replace(' ', '_')}.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
