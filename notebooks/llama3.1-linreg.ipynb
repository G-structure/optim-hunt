{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "from optim_hunter.plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "#device = t.device(\"cuda:0,1\" if t.cuda.is_available() else \"cpu\")\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# device = t.device(\"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directly from model path https://github.com/TransformerLensOrg/TransformerLens/issues/691\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "MODEL_PATH = \"/home/freiza/optim_hunter/.models/Llama-3.1-8B-Instruct/\"\n",
    "\n",
    "if MODEL_PATH:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True,\n",
    "                                                     #quantization_config=BitsAndBytesConfig(load_in_4bit=True), \n",
    "                                                     #torch_dtype = t.float32, \n",
    "                                                     #device_map = \"cuda:0\"\n",
    "                                                     )\n",
    "\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        MODEL_TYPE,\n",
    "        hf_model=hf_model,\n",
    "        device=\"cuda\",\n",
    "        n_devices=2,\n",
    "        fold_ln=False,\n",
    "        fold_value_biases=False,\n",
    "        enter_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        dtype=t.bfloat16,\n",
    "        default_padding_side='left',\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    #model = model.to(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "    #model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f49fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(x_train, y_train, x_test):\n",
    "    \"\"\"\n",
    "    Prepare the prompt without using LangChain while maintaining exact same format\n",
    "    \"\"\"\n",
    "    # Get input variables (features)\n",
    "    input_variables = x_train.columns.to_list()\n",
    "\n",
    "    # Create examples list of dicts combining x and y values\n",
    "    examples = [{**x1, y_train.name: x2} for x1, x2 in zip(x_train.to_dict('records'), y_train)]\n",
    "\n",
    "    # Create the template for examples\n",
    "    template = [f\"{feature}: {{{feature}}}\" for feature in x_train.columns]\n",
    "    template.append(f\"{y_train.name}: {{{y_train.name}}}\")\n",
    "    template = \"\\n\".join(template)\n",
    "\n",
    "    # Create suffix (test case format)\n",
    "    suffix = [f\"{feature}: {{{feature}}}\" for feature in x_train.columns]\n",
    "    suffix.append(f\"{y_train.name}:\")\n",
    "    suffix = \"\\n\".join(suffix)\n",
    "\n",
    "    # Format all examples using the template\n",
    "    formatted_examples = [template.format(**example) for example in examples]\n",
    "    examples_text = \"\\n\\n\".join(formatted_examples)\n",
    "\n",
    "    # Format the test case using the suffix\n",
    "    test_case = suffix.format(**x_test.to_dict('records')[0])\n",
    "\n",
    "    # Add instruction prefix\n",
    "    prefix_instruction = 'The task is to provide your best estimate for \"Output\". Please provide that and only that, without any additional text.\\n\\n\\n\\n\\n'\n",
    "\n",
    "    # Combine everything\n",
    "    final_prompt = f\"{prefix_instruction}{examples_text}\\n\\n{test_case}\"\n",
    "\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0465c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "def linear_regression(x_train, x_test, y_train, y_test, random_state=1):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_test    = y_test.to_numpy()\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "def mlp(x_train, x_test, y_train, y_test, random_state=1):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron\n",
    "    \"\"\"\n",
    "    model = MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='lbfgs', random_state=random_state)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_test    = y_test.to_numpy()\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "def gradient_boosting(x_train, x_test, y_train, y_test, random_state=1):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regressor\n",
    "    \"\"\"\n",
    "    model = GradientBoostingRegressor(random_state=random_state)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_test    = y_test.to_numpy()\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "def random_forest(x_train, x_test, y_train, y_test, random_state=1):\n",
    "    \"\"\"\n",
    "    Random Forest Regressor\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(max_depth=3, random_state=random_state)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_test    = y_test.to_numpy()\n",
    "\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51217a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "## Friedman #2 Dataset\n",
    "##############################\n",
    "# Here, we will use Friedman #2\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_dataset1(random_state=1):\n",
    "\n",
    "    # The data from sklearn\n",
    "    r_data, r_values = make_friedman2(n_samples=51, noise=0, random_state=random_state)\n",
    "\n",
    "    # Create a dataframe; Not mandatory, but makes things easier\n",
    "    df = pd.DataFrame({**{f'Feature {i}': r_data[:, i] for i in range(r_data.shape[1])}, 'Output': r_values})\n",
    "    x = df.drop(['Output'], axis=1)\n",
    "    y = df['Output']\n",
    "\n",
    "    # Round the values to 2 decimal places\n",
    "    # Not mandatory, but helps to: (1) Keep the costs low, (2) Work with the same numbers of examples with models that have a smaller context (e.g., Yi, Llama, etc)\n",
    "    x = np.round(x, 2)\n",
    "    y = np.round(y, 2)\n",
    "\n",
    "    # Do a random split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1, random_state=random_state)\n",
    "\n",
    "\n",
    "    x_train = x_train.iloc[:50]\n",
    "    y_train = y_train.iloc[:50]\n",
    "    x_test  = x_test.iloc[:1]\n",
    "    y_test  = y_test.iloc[:1]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b89675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(x_train, y_train, x_test, y_test, n=10):\n",
    "    \"\"\"\n",
    "    Slice the first n items from each dataset while preserving DataFrame structure\n",
    "    \n",
    "    Args:\n",
    "        x_train (pd.DataFrame): Training features\n",
    "        y_train (pd.Series): Training labels\n",
    "        x_test (pd.DataFrame): Test features\n",
    "        y_test (pd.Series): Test labels\n",
    "        n (int): Number of items to keep\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (x_train_slice, y_train_slice, x_test_slice, y_test_slice)\n",
    "    \"\"\"\n",
    "    x_train_slice = x_train.iloc[:n]\n",
    "    y_train_slice = y_train.iloc[:n]\n",
    "    x_test_slice = x_test.iloc[:n]\n",
    "    y_test_slice = y_test.iloc[:n]\n",
    "    \n",
    "    return x_train_slice, y_train_slice, x_test_slice, y_test_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91e6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linreg_tokens(\n",
    "    model: HookedTransformer,\n",
    "    dataset,\n",
    "    seq_len = 5,\n",
    "    batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    '''\n",
    "    Generates a sequence of linear regression ICL tokens\n",
    "\n",
    "    Outputs are:\n",
    "        linreg_tokens: [batch, 1+linreg]\n",
    "    '''\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long().to(device)\n",
    "    \n",
    "    # Create list to store tokens for each batch\n",
    "    batch_tokens = []\n",
    "    \n",
    "    # Generate tokens for each batch with different random seeds\n",
    "    for i in range(batch):\n",
    "        # TODO fix so that we can use random_state=i\n",
    "        x_train, y_train, x_test, y_test = get_dataset1(random_state=1)\n",
    "        x_train, y_train, x_test, y_test =  slice_dataset(x_train, y_train, x_test, y_test, seq_len)\n",
    "        prompt = prepare_prompt(x_train, y_train, x_test)\n",
    "        tokens = model.to_tokens(prompt, truncate=True)\n",
    "        batch_tokens.append(tokens[0])\n",
    "    \n",
    "    # Stack all batches together\n",
    "    linreg_tokens = t.stack(batch_tokens).to(device)\n",
    "    \n",
    "    # Add prefix to each batch\n",
    "    linreg_tokens = t.cat([prefix, linreg_tokens], dim=-1).to(device)\n",
    "    return linreg_tokens\n",
    "\n",
    "def run_and_cache_model_linreg_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "    '''\n",
    "    Generates a sequence of linear regression ICL tokens, and runs the model on it, returning (tokens, logits, cache)\n",
    "\n",
    "    Should use the `generate_linreg_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        linreg_tokens: [batch, 1+linreg]\n",
    "        linreg_logits: [batch, 1+linreg, d_vocab]\n",
    "        linreg_cache: The cache of the model run on linreg_tokens\n",
    "    '''\n",
    "    linreg_tokens = generate_linreg_tokens(model, get_dataset1, seq_len, batch)\n",
    "    linreg_logits, linreg_cache = model.run_with_cache(linreg_tokens)\n",
    "    return linreg_tokens, linreg_logits, linreg_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c0135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "batch = 1\n",
    "(linreg_tokens, linreg_logits, linreg_cache) = run_and_cache_model_linreg_tokens(model, seq_len, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbabc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pattern_hook(\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(\n",
    "        cv.attention.attention_patterns(\n",
    "            tokens=model.to_str_tokens(linreg_tokens[0]),\n",
    "            attention=pattern.mean(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "for induction_head_layer in range(model.cfg.n_layers):\n",
    "    model.run_with_hooks(\n",
    "        linreg_tokens,\n",
    "        return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[\n",
    "            (utils.get_act_name(\"pattern\", induction_head_layer), visualize_pattern_hook)\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
